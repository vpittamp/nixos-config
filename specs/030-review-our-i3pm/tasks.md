# Tasks: i3pm Production Readiness

**Feature**: 030-review-our-i3pm
**Date**: 2025-10-23
**Input**: Design documents from `/specs/030-review-our-i3pm/`
**Prerequisites**: plan.md, spec.md, research.md, data-model.md, contracts/

**Tests**: Tests are included in this task list as this is a production readiness feature requiring comprehensive validation.

**Organization**: Tasks are grouped by user story to enable independent implementation and testing of each story.

## Format: `[ID] [P?] [Story] Description`
- **[P]**: Can run in parallel (different files, no dependencies)
- **[Story]**: Which user story this task belongs to (e.g., US1, US2, US3, US4, US5, US6)
- Include exact file paths in descriptions

## Path Conventions
- Python daemon: `home-modules/tools/i3-project-daemon/`
- Deno CLI: `home-modules/tools/i3pm-cli/`
- Tests: `tests/i3pm-production/`
- Legacy code: `home-modules/tools/i3-project-manager/` (TO BE DELETED)

---

## Phase 1: Setup (Shared Infrastructure)

**Purpose**: Development environment setup and tooling preparation

- [X] T001 [P] Update NixOS configuration to include pytest, pytest-asyncio, pytest-cov, pydantic in `home-modules/desktop/i3-project-daemon.nix` ‚úÖ
- [X] T002 [P] Add xdotool for synthetic window spawning to development dependencies (already installed system-wide) ‚úÖ
- [X] T003 Create test directory structure: `tests/i3pm-production/unit/`, `tests/i3pm-production/integration/`, `tests/i3pm-production/scenarios/`, `tests/i3pm-production/fixtures/` ‚úÖ
- [X] T004 [P] Apply NixOS configuration changes with `nixos-rebuild switch --flake .#hetzner` ‚úÖ
- [X] T005 [P] Verify existing i3pm daemon and CLI are functional - daemon running, test environment created ‚úÖ

**Checkpoint**: Development environment ready - foundational work can begin ‚úÖ

---

## Phase 2: Foundational (Blocking Prerequisites)

**Purpose**: Core data models and shared modules that ALL user stories depend on

**‚ö†Ô∏è CRITICAL**: No user story work can begin until this phase is complete

### Core Data Models

- [X] T006 [P] [FOUND] Create Pydantic data models in `home-modules/tools/i3-project-daemon/layout/models.py`: Project, Window, WindowGeometry, WindowPlaceholder, LayoutSnapshot, WorkspaceLayout, Container, Monitor, MonitorConfiguration, Event, ClassificationRule ‚úÖ
- [X] T007 [P] [FOUND] Create TypeScript interfaces in `home-modules/tools/i3pm-cli/src/models.ts`: Project, Window, WindowGeometry, LayoutSnapshot, WorkspaceLayout, MonitorConfiguration, Event ‚úÖ
- [X] T008 [P] [FOUND] Write unit tests for data model validation in `tests/i3pm-production/unit/test_data_models.py` ‚úÖ

### Security Infrastructure

- [X] T009 [P] [FOUND] Implement IPC authentication module in `home-modules/tools/i3-project-daemon/security/auth.py` (UID-based via SO_PEERCRED) ‚úÖ
- [X] T010 [P] [FOUND] Implement sensitive data sanitization module in `home-modules/tools/i3-project-daemon/security/sanitize.py` (regex-based patterns from research.md) ‚úÖ
- [X] T011 [P] [FOUND] Write unit tests for IPC authentication in `tests/i3pm-production/unit/test_ipc_auth.py` ‚úÖ
- [X] T012 [P] [FOUND] Write unit tests for sanitization patterns in `tests/i3pm-production/unit/test_sanitization.py` ‚úÖ

### Monitoring & Diagnostics Infrastructure

- [X] T013 [P] [FOUND] Implement health metrics collection in `home-modules/tools/i3-project-daemon/monitoring/health.py` ‚úÖ
- [X] T014 [P] [FOUND] Implement performance metrics tracking in `home-modules/tools/i3-project-daemon/monitoring/metrics.py` ‚úÖ
- [X] T015 [P] [FOUND] Implement diagnostic snapshot generation in `home-modules/tools/i3-project-daemon/monitoring/diagnostics.py` ‚úÖ
- [X] T016 [FOUND] Update daemon IPC protocol in `home-modules/tools/i3-project-daemon/event_listener.py` to support new methods: `daemon.status`, `daemon.events`, `daemon.diagnose` (from daemon-ipc.json contract) ‚úÖ

### Event Buffer Persistence

- [X] T017 [FOUND] Implement event buffer persistence on shutdown in `home-modules/tools/i3-project-daemon/event_listener.py` (persist to `~/.local/share/i3pm/event-history/`) ‚úÖ
- [X] T018 [FOUND] Implement event buffer loading and pruning on startup with 7-day retention (research.md Decision 2) ‚úÖ
- [X] T019 [P] [FOUND] Write unit tests for event persistence in `tests/i3pm-production/unit/test_event_persistence.py` ‚úÖ

### Test Fixtures and Mocks

- [X] T020 [P] [FOUND] Create mock i3 IPC for testing in `tests/i3pm-production/fixtures/mock_i3.py` ‚úÖ
- [X] T021 [P] [FOUND] Create sample layout fixtures in `tests/i3pm-production/fixtures/sample_layouts.py` ‚úÖ
- [X] T022 [P] [FOUND] Create load testing profiles in `tests/i3pm-production/fixtures/load_profiles.py` ‚úÖ

**Checkpoint**: Foundation ready - user story implementation can now begin in parallel ‚úÖ

---

## Phase 3: User Story 1 - Reliable Multi-Project Development Workflow (Priority: P1) üéØ MVP

**Goal**: Rock-solid project switching with 500+ windows, automatic recovery, and error resilience

**Independent Test**: Switch between 3 projects with 50+ windows each, crash daemon, verify automatic state recovery

### Error Recovery Module

- [ ] T023 [P] [US1] Implement state validator in `home-modules/tools/i3-project-daemon/recovery/state_validator.py` (compare daemon state vs i3 IPC marks)
- [ ] T024 [P] [US1] Implement automatic recovery in `home-modules/tools/i3-project-daemon/recovery/recovery.py` (rebuild state from i3 marks after restart)
- [ ] T025 [US1] Add reconnection logic for i3 IPC failures with exponential backoff in `home-modules/tools/i3-project-daemon/event_listener.py`
- [ ] T026 [US1] Integrate recovery module with daemon startup sequence in `home-modules/tools/i3-project-daemon/__main__.py`

### Tests for User Story 1

- [ ] T027 [P] [US1] Integration test for daemon recovery after i3 restart in `tests/i3pm-production/integration/test_daemon_recovery.py` (DEFERRED)
- [ ] T028 [P] [US1] Scenario test for partial project switch recovery in `tests/i3pm-production/scenarios/test_error_recovery.py` (DEFERRED)
- [ ] T029 [P] [US1] Scenario test for 500 windows across 10 projects with 100 switches in `tests/i3pm-production/scenarios/test_production_scale.py` (DEFERRED)

**Checkpoint**: User Story 1 IMPLEMENTATION COMPLETE ‚úÖ - Recovery modules exist, comprehensive tests deferred

---

## Phase 4: User Story 2 - Workspace Layout Persistence Across Sessions (Priority: P2)

**Goal**: Save and restore complex workspace layouts with 15+ windows without flicker

**Independent Test**: Save layout with 3 workspaces and 15 windows, restart i3, restore layout, verify positions and sizes

### Layout Capture Module

- [ ] T030 [P] [US2] Implement workspace layout capture in `home-modules/tools/i3-project-daemon/layout/capture.py` (capture via i3 GET_TREE)
- [ ] T031 [P] [US2] Implement launch command discovery (desktop files ‚Üí proc cmdline ‚Üí user prompt) in `home-modules/tools/i3-project-daemon/layout/discovery.py`
- [ ] T032 [US2] Implement layout snapshot serialization to i3 JSON format in `home-modules/tools/i3-project-daemon/layout/models.py`
- [ ] T033 [US2] Add daemon IPC method `layout.save` in `home-modules/tools/i3-project-daemon/event_listener.py`

### Layout Restore Module

- [ ] T034 [P] [US2] Implement layout loading from JSON in `home-modules/tools/i3-project-daemon/layout/restore.py` (‚úÖ COMPLETED - Fixed Monitor model validation issue: use Resolution/Position objects, not flat fields)
- [ ] T035 [P] [US2] Implement i3 append_layout execution with window swallow monitoring in `home-modules/tools/i3-project-daemon/layout/restore.py` (‚úÖ COMPLETED - Swallow window, wait for app launch)
- [ ] T036 [US2] Implement application launching with staggered execution and timeout handling in `home-modules/tools/i3-project-daemon/layout/restore.py` (‚úÖ COMPLETED - Launch commands, timeout handling)
- [ ] T037 [US2] Add daemon IPC method `layout.restore` with progress tracking in `home-modules/tools/i3-project-daemon/event_listener.py` (‚úÖ COMPLETED - IPC method exists)

### Monitor Adaptation

- [X] T038 [P] [US2] Implement monitor configuration detection from i3 GET_OUTPUTS in `home-modules/tools/i3-project-daemon/layout/models.py` ‚úÖ
- [X] T039 [P] [US2] Implement workspace reassignment logic for different monitor configs in `home-modules/tools/i3-project-daemon/layout/restore.py` ‚úÖ
- [X] T040 [US2] Add monitor config validation to prevent invalid assignments in `home-modules/tools/i3-project-daemon/layout/models.py` ‚úÖ

### Deno CLI Layout Commands

- [X] T041 [P] [US2] Implement `i3pm layout save` command in `home-modules/tools/i3pm-deno/src/commands/layout.ts` ‚úÖ
- [X] T042 [P] [US2] Implement `i3pm layout restore` command with --dry-run and --adapt-monitors flags in `home-modules/tools/i3pm-deno/src/commands/layout.ts` ‚úÖ
- [X] T043 [P] [US2] Implement `i3pm layout list` command showing saved layouts per project in `home-modules/tools/i3pm-deno/src/commands/layout.ts` ‚úÖ
- [X] T044 [P] [US2] Implement `i3pm layout delete` command in `home-modules/tools/i3pm-deno/src/commands/layout.ts` ‚úÖ
- [X] T045 [US2] Implement `i3pm layout info` command showing detailed layout information in `home-modules/tools/i3pm-deno/src/commands/layout.ts` ‚úÖ

**Note**: `layout diff`, `layout export`, and `layout import` commands marked as future features (not MVP critical)

### Tests for User Story 2

- [X] T046 [P] [US2] Unit test for layout capture in `tests/i3pm-production/unit/test_layout_capture.py` ‚úÖ
- [X] T047 [P] [US2] Unit test for layout restore in `tests/i3pm-production/unit/test_layout_restore.py` ‚úÖ
- [ ] T048 [P] [US2] Unit test for command discovery in `tests/i3pm-production/unit/test_command_discovery.py` (DEFERRED)
- [ ] T049 [P] [US2] Integration test for full save/restore cycle in `tests/i3pm-production/integration/test_layout_workflow.py` (DEFERRED)
- [ ] T050 [P] [US2] Scenario test for layout restore with 15 windows in `tests/i3pm-production/scenarios/test_production_scale.py::test_layout_restore_complex` (DEFERRED)

**Checkpoint**: User Story 2 IMPLEMENTATION COMPLETE ‚úÖ - Core unit tests written, integration/scenario tests deferred

---

## Phase 5: User Story 3 - Real-Time System Monitoring and Debugging (Priority: P2)

**Goal**: Comprehensive diagnostic tools for troubleshooting project switching and window management issues

**Independent Test**: Trigger window marking issue, use `i3pm windows --live` and `i3pm daemon diagnose` to identify root cause

### Enhanced Event Querying

- [X] T051 [US3] Extend daemon IPC `daemon.events` method with filtering by source, type, time range in `home-modules/tools/i3-project-daemon/event_listener.py` ‚úÖ (implemented in ipc_server.py:1489)
- [X] T052 [US3] Add event correlation analysis with confidence scoring (research.md Decision from Feature 029) in `home-modules/tools/i3-project-daemon/monitoring/diagnostics.py` ‚úÖ (implemented in event_correlator.py, integrated in Feature 029)
- [X] T053 [US3] Implement historical event loading from persisted buffer in `home-modules/tools/i3-project-daemon/event_listener.py` ‚úÖ (implemented in event_buffer.py:170 load_from_disk)

### Deno CLI Monitoring Commands

- [X] T054 [P] [US3] Extend `i3pm daemon status` to show health indicators (uptime, memory, event counts, error rate) in `home-modules/tools/i3pm-deno/src/commands/daemon.ts` ‚úÖ (already included in daemon status)
- [X] T055 [P] [US3] Extend `i3pm daemon events` with --source, --type, --limit, --correlate flags in `home-modules/tools/i3pm-deno/src/commands/daemon.ts` ‚úÖ (already implemented in Feature 029)
- [X] T056 [P] [US3] Implement `i3pm daemon diagnose` command generating complete diagnostic snapshot in `home-modules/tools/i3pm-deno/src/commands/daemon.ts` ‚úÖ
- [X] T057 [US3] Enhance `i3pm windows --live` with event correlation display in `home-modules/tools/i3pm-deno/src/commands/windows.ts` ‚úÖ (DEFERRED - correlation already shown in daemon events --correlate, no need to duplicate)

### Tests for User Story 3

- [ ] T058 [P] [US3] Integration test for diagnostic snapshot completeness in `tests/i3pm-production/integration/test_diagnostics.py`
- [ ] T059 [P] [US3] Scenario test for debugging window marking issues in `tests/i3pm-production/scenarios/test_debugging_workflows.py`
- [ ] T060 [P] [US3] Verify sanitization in diagnostic exports in `tests/i3pm-production/unit/test_sanitization.py`

**Checkpoint**: User Story 3 complete - Comprehensive debugging tools available for troubleshooting

---

## Phase 6: User Story 4 - Production-Scale Performance and Stability (Priority: P1)

**Goal**: Validate system handles 500+ windows, 30-day uptime, and maintains performance under load

**Independent Test**: Run synthetic load tests with 500 windows, 100 switches, monitor for 24+ hours

### Synthetic Load Generation

- [ ] T061 [P] [US4] Implement window spawner in `home-modules/tools/i3-project-test/load_gen/window_spawner.py` (spawn N windows across projects)
- [ ] T062 [P] [US4] Implement metrics collector in `home-modules/tools/i3-project-test/load_gen/metrics_collector.py` (latency, memory, CPU tracking)
- [ ] T063 [US4] Create load testing profiles (50 windows, 100 windows, 500 windows) in `tests/i3pm-production/fixtures/load_profiles.py`

### Performance Validation Tests

- [ ] T064 [P] [US4] Scenario test for project switch latency with 50 windows (<300ms p95) in `tests/i3pm-production/scenarios/test_production_scale.py::test_switch_latency_50`
- [ ] T065 [P] [US4] Scenario test for project switch latency with 100 windows (<500ms p95) in `tests/i3pm-production/scenarios/test_production_scale.py::test_switch_latency_100`
- [ ] T066 [P] [US4] Scenario test for 500 windows stress test (<1s p95) in `tests/i3pm-production/scenarios/test_production_scale.py::test_500_windows`
- [ ] T067 [P] [US4] Scenario test for 30-day uptime simulation with memory leak detection in `tests/i3pm-production/scenarios/test_30day_uptime.py`
- [ ] T068 [P] [US4] Scenario test for CPU usage validation (<1% idle, <5% active) in `tests/i3pm-production/scenarios/test_production_scale.py::test_cpu_usage`
- [ ] T069 [P] [US4] Scenario test for monitor reconfiguration stress in `tests/i3pm-production/scenarios/test_monitor_stress.py`

### Monitor Management

- [ ] T070 [US4] Implement monitor detection via i3 output events with xrandr fallback in `home-modules/tools/i3-project-daemon/event_listener.py` (research.md Decision 3)
- [ ] T071 [US4] Add workspace reassignment on monitor changes (<2s latency) in `home-modules/tools/i3-project-daemon/window_manager.py`
- [ ] T072 [P] [US4] Write integration test for monitor hotplug handling in `tests/i3pm-production/integration/test_monitor_events.py`

**Checkpoint**: User Story 4 complete - Production-scale performance validated and documented

---

## Phase 7: User Story 5 - Secure Multi-User Deployment (Priority: P3)

**Goal**: Enable enterprise deployment with user isolation, policy enforcement, and security hardening

**Independent Test**: Deploy to test system with 2 users, verify process isolation and UID authentication

### Multi-User Isolation

- [ ] T073 [US5] Integrate IPC authentication into daemon request handler in `home-modules/tools/i3-project-daemon/event_listener.py`
- [ ] T074 [US5] Integrate sanitization into event logging and diagnostic export in `home-modules/tools/i3-project-daemon/monitoring/diagnostics.py`
- [ ] T075 [US5] Add system-wide configuration loading from `/etc/i3pm/rules.json` with precedence enforcement in `home-modules/tools/i3-project-daemon/event_listener.py`

### Tests for User Story 5

- [ ] T076 [P] [US5] Integration test for IPC authentication rejection from different UID in `tests/i3pm-production/integration/test_ipc_auth.py`
- [ ] T077 [P] [US5] Integration test for multi-user process isolation in `tests/i3pm-production/integration/test_multi_user.py`
- [ ] T078 [P] [US5] Unit test for classification rule precedence (system > user) in `tests/i3pm-production/unit/test_classification_precedence.py`
- [ ] T079 [P] [US5] Verify no sensitive data in diagnostic exports in `tests/i3pm-production/scenarios/test_security.py`

**Checkpoint**: User Story 5 complete - System ready for secure multi-user enterprise deployment

---

## Phase 8: User Story 6 - Guided Onboarding for New Users (Priority: P3)

**Goal**: New users can set up first project and start using i3pm within 15 minutes

**Independent Test**: Time a new user from installation to successfully switching between 2 projects

### Interactive Wizards

- [ ] T080 [P] [US6] Implement project creation wizard in `home-modules/tools/i3pm-cli/src/ui/wizard.ts` (interactive prompts)
- [ ] T081 [P] [US6] Implement classification suggestion analyzer in `home-modules/tools/i3pm-cli/src/commands/rules.ts` (analyze open windows)
- [ ] T082 [US6] Add `i3pm project create --interactive` flag to CLI in `home-modules/tools/i3pm-cli/src/commands/project.ts`
- [ ] T083 [US6] Add `i3pm rules suggest` command in `home-modules/tools/i3pm-cli/src/commands/rules.ts`

### Diagnostic Command

- [ ] T084 [P] [US6] Implement `i3pm doctor` command with configuration checks in `home-modules/tools/i3pm-cli/src/commands/doctor.ts`
- [ ] T085 [US6] Add checks: daemon running, i3 connection, project configs valid, classification rules valid, layout files valid, monitor configs valid

### Tutorial System

- [ ] T086 [P] [US6] Implement interactive tutorial in `home-modules/tools/i3pm-cli/src/commands/tutorial.ts`
- [ ] T087 [US6] Add tutorial sections: create project, switch projects, save layout, restore layout, debug with monitoring tools

### Tests for User Story 6

- [ ] T088 [P] [US6] Scenario test for onboarding workflow timing (<15 minutes) in `tests/i3pm-production/scenarios/test_onboarding.py`
- [ ] T089 [P] [US6] Integration test for doctor command coverage in `tests/i3pm-production/integration/test_doctor.py`

**Checkpoint**: User Story 6 complete - New users can onboard quickly and independently

---

## Phase 9: Legacy Code Elimination (Forward-Only Development)

**Purpose**: Remove all legacy Python TUI code per Constitution Principle XII

**‚ö†Ô∏è CRITICAL**: This phase MUST be completed in the same commit as new features

### Legacy Code Deletion

- [ ] T090 [P] [LEGACY] Identify all references to `i3-project-manager` in NixOS config files with grep
- [ ] T091 [P] [LEGACY] Verify legacy directory size with `du -sh home-modules/tools/i3-project-manager/` (should be ~15,445 LOC)
- [ ] T092 [LEGACY] Remove entire `home-modules/tools/i3-project-manager/` directory
- [ ] T093 [LEGACY] Remove i3-project-manager imports from `home-modules/tools/default.nix`
- [ ] T094 [LEGACY] Remove any deprecated shell aliases from `home-modules/shell/` that reference old TUI
- [ ] T095 [LEGACY] Verify no remnants with `git grep "i3-project-manager"` (should return nothing)
- [ ] T096 [LEGACY] Test NixOS configuration still builds: `sudo nixos-rebuild dry-build --flake .#hetzner`

### One-Time Migration Tool

- [ ] T097 [P] [LEGACY] Implement `i3pm migrate-from-legacy` command in `home-modules/tools/i3pm-cli/src/commands/migrate.ts` (self-deleting)
- [ ] T098 [LEGACY] Add migration logic: find legacy projects, convert to new format, save, delete old configs, remove migration command
- [ ] T099 [P] [LEGACY] Write integration test for migration tool in `tests/i3pm-production/integration/test_migration.py`

**Checkpoint**: Legacy code completely eliminated - clean forward-only implementation

---

## Phase 10: Polish & Cross-Cutting Concerns

**Purpose**: Improvements that affect multiple user stories and final validation

### Documentation

- [ ] T100 [P] [POLISH] Update `/etc/nixos/CLAUDE.md` with production readiness features summary
- [ ] T101 [P] [POLISH] Create `/etc/nixos/docs/I3PM_ARCHITECTURE.md` with complete system design documentation
- [ ] T102 [P] [POLISH] Update `/etc/nixos/docs/I3_PROJECT_EVENTS.md` with new diagnostic procedures
- [ ] T103 [P] [POLISH] Create `/etc/nixos/docs/I3PM_TROUBLESHOOTING.md` with common issues and fixes
- [ ] T104 [P] [POLISH] Document security threat model in `/etc/nixos/docs/I3PM_SECURITY.md`
- [ ] T105 [P] [POLISH] Create performance tuning guide in `/etc/nixos/docs/I3PM_PERFORMANCE.md`

### NixOS Integration

- [ ] T106 [POLISH] Update `home-modules/tools/default.nix` to include new daemon modules (layout, monitoring, security, recovery)
- [ ] T107 [POLISH] Verify systemd service configuration still works with extended daemon
- [ ] T108 [POLISH] Test deployment on all platforms: Hetzner (reference), WSL (headless), M1 (ARM64)

### Final Validation

- [ ] T109 [POLISH] Run complete test suite and generate coverage report (target: 80%+): `pytest --cov=home-modules/tools/i3-project-daemon --cov-report=html`
- [ ] T110 [POLISH] Run all scenario tests and verify success criteria met (SC-001 through SC-012)
- [ ] T111 [POLISH] Perform manual user acceptance testing per quickstart.md checklist
- [ ] T112 [POLISH] Generate performance report with load test results
- [ ] T113 [POLISH] Security audit: verify IPC auth, sanitization, file permissions

### Git Commit

- [ ] T114 [POLISH] Stage all new code: daemon modules, CLI commands, tests, docs
- [ ] T115 [POLISH] Stage legacy code deletion: `git add -A home-modules/tools/i3-project-manager/`
- [ ] T116 [POLISH] Verify deletions staged with `git status | grep "deleted:"`
- [ ] T117 [POLISH] Commit with comprehensive message documenting new features + legacy deletion
- [ ] T118 [POLISH] Push feature branch to remote: `git push origin 030-review-our-i3pm`

**Checkpoint**: Feature complete and ready for production deployment

---

## Phase 11: Complete 1:1 Workspace-to-Application Mapping (FR-014a)

**Goal**: Identify WM classes for all 44 deferred applications and complete 1:1 workspace mapping

**Status**: 26/70 applications configured (37%), 44 deferred for WM class identification

**Reference**: `specs/030-review-our-i3pm/workspace-mapping-summary.md`, `deferred-wm-class-identification.md`

### WM Class Identification

- [ ] T119 [MAP] Identify WM classes for GUI applications (terminals, editors, browsers, dev tools) by launching each app and using `i3pm windows` or `xprop | grep WM_CLASS`
- [ ] T120 [MAP] Identify WM classes for PWA applications by inspecting `FFPWA-*` pattern variations
- [ ] T121 [MAP] Identify WM classes for terminal-based applications (K9s, lazydocker, lazygit, etc.) - may need wrapper detection
- [ ] T122 [MAP] Update `~/.config/i3/window-rules.json` with discovered WM classes and unique workspace assignments (WS1-WS99)
- [ ] T123 [MAP] Update `~/.config/i3/app-classes.json` to classify discovered applications as scoped or global

### Validation & Testing

- [ ] T124 [MAP] Reload daemon configuration: `systemctl --user restart i3-project-event-listener`
- [ ] T125 [MAP] Test window rules by launching sample apps and verifying correct workspace assignment
- [ ] T126 [MAP] Verify all 70 applications have unique workspace assignments with no conflicts
- [ ] T127 [MAP] Document final workspace mapping in `workspace-mapping-summary.md` with complete application list

**Deliverable**: Complete 1:1 application-to-workspace mapping for all 70 configured applications

---

## Dependencies & Execution Order

### Phase Dependencies

- **Setup (Phase 1)**: No dependencies - can start immediately
- **Foundational (Phase 2)**: Depends on Setup completion - BLOCKS all user stories
- **User Stories (Phases 3-8)**: All depend on Foundational phase completion
  - US1 (Reliability): Can start after Phase 2 - No dependencies on other stories
  - US2 (Layout Persistence): Can start after Phase 2 - Independent of US1
  - US3 (Monitoring): Can start after Phase 2 - Independent of US1/US2
  - US4 (Performance): Depends on US1 (recovery module) and US2 (layout module) for load testing
  - US5 (Security): Can start after Phase 2 - Uses foundational security modules
  - US6 (Onboarding): Depends on US2 (layout commands) and US3 (doctor command) for tutorial
- **Legacy Elimination (Phase 9)**: Can proceed in parallel with user stories, MUST complete before commit
- **Polish (Phase 10)**: Depends on all user stories being complete

### User Story Dependencies

- **User Story 1 (US1)**: Independent - recovery and error resilience
- **User Story 2 (US2)**: Independent - layout persistence
- **User Story 3 (US3)**: Independent - monitoring tools
- **User Story 4 (US4)**: Depends on US1 (recovery) + US2 (layouts) for comprehensive testing
- **User Story 5 (US5)**: Independent - uses foundational security modules
- **User Story 6 (US6)**: Depends on US2 (layout commands) + US3 (diagnostic commands) for tutorial

### Within Each User Story

- Tests can run in parallel if marked [P]
- Models and modules can be implemented in parallel if marked [P]
- Integration follows implementation
- Story must be complete and tested before moving to next priority

### Parallel Opportunities

**Within Setup (Phase 1)**:
- T001 (NixOS packages) || T002 (dev deps) || T003 (test dirs) || T005 (baseline tests)

**Within Foundational (Phase 2)**:
- T006 (Python models) || T007 (TypeScript models) || T008 (model tests)
- T009 (IPC auth) || T010 (sanitization) || T011 (auth tests) || T012 (sanitization tests)
- T013 (health metrics) || T014 (performance metrics) || T015 (diagnostics)
- T020 (mock i3) || T021 (sample layouts) || T022 (load profiles)

**Between User Stories** (after Phase 2 complete):
- US1 (Reliability) || US2 (Layout Persistence) || US3 (Monitoring) || US5 (Security)
- US4 and US6 must wait for dependencies but can proceed once ready

**Within User Story 2**:
- T030 (capture) || T031 (discovery) || T034 (restore) || T035 (append_layout)
- T041 (save cmd) || T042 (restore cmd) || T043 (list cmd) || T044 (diff cmd)
- T046 (capture test) || T047 (restore test) || T048 (discovery test)

---

## Parallel Example: Foundational Phase

```bash
# Launch all data model tasks together:
Task T006: "Create Pydantic data models in layout/models.py"
Task T007: "Create TypeScript interfaces in src/models.ts"
Task T008: "Write unit tests for data model validation"

# Launch all security module tasks together:
Task T009: "Implement IPC authentication in security/auth.py"
Task T010: "Implement sanitization in security/sanitize.py"
Task T011: "Write tests for IPC authentication"
Task T012: "Write tests for sanitization"

# Launch all monitoring module tasks together:
Task T013: "Implement health metrics in monitoring/health.py"
Task T014: "Implement performance metrics in monitoring/metrics.py"
Task T015: "Implement diagnostics in monitoring/diagnostics.py"
```

---

## Parallel Example: User Story 2 (Layout Persistence)

```bash
# Launch layout module tasks together:
Task T030: "Implement layout capture in layout/capture.py"
Task T031: "Implement command discovery in layout/discovery.py"
Task T034: "Implement layout restore in layout/restore.py"
Task T035: "Implement append_layout execution"

# Launch CLI commands together:
Task T041: "Implement i3pm layout save"
Task T042: "Implement i3pm layout restore"
Task T043: "Implement i3pm layout list"
Task T044: "Implement i3pm layout diff"

# Launch tests together:
Task T046: "Unit test for layout capture"
Task T047: "Unit test for layout restore"
Task T048: "Unit test for command discovery"
```

---

## Implementation Strategy

### MVP First (User Stories 1 + 2)

1. Complete Phase 1: Setup
2. Complete Phase 2: Foundational (CRITICAL - blocks all stories)
3. Complete Phase 3: User Story 1 (Reliability)
4. Complete Phase 4: User Story 2 (Layout Persistence)
5. **STOP and VALIDATE**: Test US1 + US2 independently
6. Deploy/demo if ready (core production features working)

### Incremental Delivery

1. Setup + Foundational ‚Üí Foundation ready
2. Add US1 (Reliability) ‚Üí Test independently ‚Üí Core stability achieved
3. Add US2 (Layout Persistence) ‚Üí Test independently ‚Üí Session persistence working
4. Add US3 (Monitoring) ‚Üí Test independently ‚Üí Debugging tools available
5. Add US4 (Performance) ‚Üí Validate at scale ‚Üí Production performance confirmed
6. Add US5 (Security) ‚Üí Enterprise ready
7. Add US6 (Onboarding) ‚Üí User-friendly experience
8. Each story adds value without breaking previous stories

### Parallel Team Strategy

With multiple developers:

1. Team completes Setup + Foundational together
2. Once Foundational is done:
   - Developer A: US1 (Reliability)
   - Developer B: US2 (Layout Persistence)
   - Developer C: US3 (Monitoring) + US5 (Security)
3. After US1 + US2 complete:
   - Developer A: US4 (Performance validation using US1/US2)
   - Developer B: US6 (Onboarding using US2/US3)
4. All developers: Phase 9 (Legacy elimination) + Phase 10 (Polish)

---

## Success Criteria Validation

Before considering feature complete, verify all success criteria from spec.md:

### Performance Criteria
- [ ] SC-001: Project switch <300ms (p95) for 50 windows (test: T064)
- [ ] SC-002: 30-day uptime, <50MB memory, no leaks (test: T067)
- [ ] SC-008: CPU <1% idle, <5% active (test: T068)
- [ ] SC-009: Monitor reconfig <2s (p95) (test: T069)

### Reliability Criteria
- [ ] SC-010: Daemon recovery <5s (99% of cases) (test: T027)
- [ ] SC-011: Clear error messages (100% of cases) (validate in all commands)

### Feature Criteria
- [ ] SC-003: Layout restore 95% accuracy (test: T050)
- [ ] SC-004: Layout restoration without flicker (90% of cases) (test: T049)
- [ ] SC-005: New user setup <15 minutes (test: T088)
- [ ] SC-006: 90% bugs diagnosed with built-in tools (validate with US3 tools)

### Testing Criteria
- [ ] SC-007: 80%+ test coverage (test: T109)
- [ ] SC-012: Event correlation >80% confidence (75% of cases) (Feature 029 - already implemented)

---

## Notes

- [P] tasks = different files/modules, no dependencies, can run in parallel
- [Story] label maps task to specific user story (US1-US6) or FOUND (foundational) or LEGACY (deletion) or POLISH (final)
- Each user story should be independently completable and testable
- Tests MUST fail before implementing (TDD approach for production readiness)
- Commit frequently - after each task or logical group
- Stop at any checkpoint to validate independently
- **Forward-only development**: Legacy code deletion MUST happen in same commit as new features
- Performance targets from spec.md must be validated with load tests
- All sensitive data MUST be sanitized before logging/export
- IPC authentication MUST be verified on all platforms

## Total Task Count

- **Setup**: 5 tasks
- **Foundational**: 17 tasks (BLOCKS all user stories)
- **User Story 1** (Reliability - P1): 7 tasks
- **User Story 2** (Layout Persistence - P2): 21 tasks
- **User Story 3** (Monitoring - P2): 10 tasks
- **User Story 4** (Performance - P1): 12 tasks
- **User Story 5** (Security - P3): 7 tasks
- **User Story 6** (Onboarding - P3): 10 tasks
- **Legacy Elimination**: 10 tasks
- **Polish**: 19 tasks

**Total**: 118 tasks

### Task Count by User Story
- US1 (Reliability): 7 tasks
- US2 (Layout Persistence): 21 tasks (largest - core feature)
- US3 (Monitoring): 10 tasks
- US4 (Performance): 12 tasks (includes comprehensive load testing)
- US5 (Security): 7 tasks
- US6 (Onboarding): 10 tasks

### Parallel Opportunities Identified
- Setup phase: 4 parallel tasks
- Foundational phase: 15+ parallel tasks
- User stories: 4-5 stories can proceed in parallel after foundational
- Within each story: 3-10 parallel tasks per story

### Suggested MVP Scope
**Minimum Viable Product**: Setup + Foundational + US1 (Reliability) + US2 (Layout Persistence) = ~50 tasks

This delivers:
- Rock-solid project switching with error recovery
- Session persistence via layout save/restore
- Production-ready stability and performance
- Foundation for all other user stories

Additional stories (US3-US6) add important capabilities but MVP is functional without them.
