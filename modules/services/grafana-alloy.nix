# Feature 129: Grafana Alloy - Unified OpenTelemetry Collector
# Feature 132: Langfuse Integration - Export traces to Langfuse for AI observability
#
# This module provides Grafana Alloy as the unified telemetry collector that:
# - Replaces otel-ai-collector.nix with a more capable solution
# - Receives OTLP telemetry from AI CLIs (Claude Code, Codex, Gemini) on port 4318
# - Forwards to otel-ai-monitor user service on port 4320 for local session tracking
# - Exports all telemetry to Kubernetes LGTM stack via Tailscale
# - Exports traces to Langfuse for AI-specific observability (Feature 132)
# - Collects system metrics via built-in node exporter
# - Collects journald logs and forwards to Loki
#
# Architecture:
#   AI CLIs → Alloy :4318 → [batch processor] → otel-ai-monitor :4320 (local)
#                                            → K8s OTEL Collector (remote)
#                                            → Langfuse OTEL endpoint (Feature 132)
#   System → node exporter → Alloy → Mimir (K8s)
#   Journald → Alloy → Loki (K8s)
#
{ config, lib, pkgs, ... }:

with lib;

let
  cfg = config.services.grafana-alloy;

  # Generate journal source blocks for each configured unit
  journalSourceBlocks = concatMapStringsSep "\n\n" (unit:
    let
      # Convert unit name to valid identifier (replace dots and hyphens)
      safeName = replaceStrings ["-" "."] ["_" "_"] (removeSuffix ".service" unit);
      serviceName = removeSuffix ".service" unit;
    in ''
      loki.source.journal "${safeName}" {
        path    = "/var/log/journal"
        matches = "_SYSTEMD_UNIT=${unit}"

        labels = {
          job     = "systemd-journal",
          service = "${serviceName}",
        }

        forward_to = [loki.relabel.add_host.receiver]
      }''
  ) cfg.journaldUnits;

  # Generate the complete Alloy configuration
  alloyConfig = pkgs.writeText "alloy-config.alloy" ''
    // Grafana Alloy Configuration
    // Generated by NixOS module: Feature 129
    // DO NOT EDIT - managed by nixos-rebuild

    // =============================================================================
    // Live Debugging - Enable debug UI at http://localhost:12345
    // =============================================================================

    livedebugging {
      enabled = true
    }

    // =============================================================================
    // OTLP Receiver - Receives telemetry from AI CLIs and other apps
    // =============================================================================

    otelcol.receiver.otlp "default" {
      http {
        endpoint = "0.0.0.0:${toString cfg.otlpPort}"
      }

      output {
        metrics = [otelcol.processor.batch.default.input]
        logs    = [otelcol.processor.batch.default.input]
        traces  = [otelcol.processor.transform.enrich_spans.input]
      }
    }

    // =============================================================================
    // Transform Processor - Enrich spans with better names/attributes
    // =============================================================================

    otelcol.processor.transform "enrich_spans" {
      error_mode = "ignore"

      trace_statements {
        context = "span"
        statements = [
          // If span name is generic "api.request" but has tool name, rename it
          "set(name, Concat([name, \" (\", attributes[\"tool.name\"], \")\"], \"\")) where attributes[\"tool.name\"] != nil",

          // Claude Code tool spans: tool.read -> Read, tool.write -> Write
          "set(name, \"Read\") where name == \"tool.read\"",
          "set(name, \"Write\") where name == \"tool.write\"",
          "set(name, \"Edit\") where name == \"tool.edit\"",
          "set(name, \"Bash\") where name == \"tool.bash\"",
          "set(name, \"Grep\") where name == \"tool.grep\"",
          "set(name, \"Glob\") where name == \"tool.glob\"",

          // Payload interceptor LLM spans: LLM -> Claude API Call (for better visibility)
          "set(name, \"Claude API Call\") where name == \"LLM\" and attributes[\"llm.provider\"] == \"anthropic\"",
          "set(name, \"Claude API Call\") where name == \"Claude Interaction (Payload)\"",

          // Enrich Claude Session root spans (multi-span trace support)
          "set(name, \"Claude Code Session\") where name == \"Claude Session\" and attributes[\"openinference.span.kind\"] == \"CHAIN\"",
        ]
      }

      output {
        traces = [otelcol.processor.filter.drop_beyla.input]
      }
    }

    // =============================================================================
    // Filter Processor - Drop Beyla auto-instrumentation traces
    //
    // Beyla generates HTTP/gRPC spans via eBPF that aren't LLM-specific.
    // Filter them out before sending to AI-focused backends (Langfuse, local monitor).
    // =============================================================================

    otelcol.processor.filter "drop_beyla" {
      error_mode = "ignore"

      traces {
        // Drop spans from Beyla (telemetry.sdk.name is a resource attribute)
        span = ["resource.attributes[\"telemetry.sdk.name\"] == \"beyla\""]
      }

      output {
        traces = [
          otelcol.processor.batch.default.input,
          otelcol.connector.spanmetrics.ai.input,
        ]
      }
    }

    // =============================================================================
    // Span Metrics - Derive metrics (+ exemplars) from spans
    //
    // Uses the spanmetrics connector to generate RED metrics from traces and attach
    // exemplars that link back to Tempo traces in Grafana.
    //
    // IMPORTANT: We exclude `span.name` to avoid high-cardinality series from
    // Turn/tool span names (which may include prompt/file previews).
    // =============================================================================

    // Dedicated batch processor to avoid cycles (spanmetrics consumes traces, emits metrics).
    otelcol.processor.batch "spanmetrics" {
      send_batch_size     = 1000
      timeout             = "10s"
      send_batch_max_size = 2000

      output {
        metrics = [otelcol.exporter.prometheus.mimir.input]
      }
    }

    otelcol.connector.spanmetrics "ai" {
      // Keep the resource key stable across runs (avoid process.pid / cwd churn).
      resource_metrics_key_attributes = ["service.name", "host.name"]

      // Avoid high-cardinality series by excluding span.name.
      exclude_dimensions = ["span.name"]

      // Useful low-cardinality dimensions for AI traces.
      dimension {
        name = "openinference.span.kind"
      }

      dimension {
        name = "gen_ai.request.model"
      }

      dimension {
        name = "gen_ai.tool.name"
      }

      histogram {
        explicit {
          buckets = ["10ms", "50ms", "100ms", "250ms", "500ms", "1s", "2s", "5s", "10s", "30s", "60s"]
        }
      }

      exemplars {
        enabled            = true
        max_per_data_point = 1
      }

      output {
        metrics = [otelcol.processor.batch.spanmetrics.input]
      }
    }

    // =============================================================================
    // Batch Processor - Buffer and batch telemetry for efficiency
    // =============================================================================

    otelcol.processor.batch "default" {
      send_batch_size     = 1000
      timeout             = "10s"
      send_batch_max_size = 2000  // Memory limit ~100MB worth of telemetry

      output {
        metrics = [
          otelcol.exporter.otlphttp.local.input,
          otelcol.exporter.prometheus.mimir.input,
        ]
        logs = [
          otelcol.exporter.otlphttp.local.input,
          otelcol.exporter.otlphttp.k8s.input,
        ]
        traces = [
          otelcol.exporter.otlphttp.local.input,
          otelcol.exporter.otlphttp.k8s.input,
          ${optionalString ((config.services ? arize-phoenix) && (config.services.arize-phoenix.enable or false)) "otelcol.exporter.otlphttp.phoenix.input,"}
          ${optionalString cfg.langfuse.enable "otelcol.exporter.otlphttp.langfuse.input,"}
        ]
      }
    }

    // =============================================================================
    // Exporters - Send to local otel-ai-monitor and remote K8s
    // =============================================================================

    // Bridge: OTLP Metrics -> Prometheus Remote Write
    otelcol.exporter.prometheus "mimir" {
      forward_to = [prometheus.remote_write.k8s.receiver]
    }

    // Local: otel-ai-monitor for EWW widgets
    otelcol.exporter.otlphttp "local" {
      client {
        endpoint = "http://localhost:${toString cfg.localForwardPort}"
        tls {
          insecure = true
        }
      }
    }

    // Local: Arize Phoenix for GenAI tracing
    ${optionalString ((config.services ? arize-phoenix) && (config.services.arize-phoenix.enable or false)) ''
    otelcol.exporter.otlphttp "phoenix" {
      client {
        endpoint = "${cfg.phoenixEndpoint}"
        tls {
          insecure = true
        }
      }
    }
    ''}

    // Remote: Kubernetes OTEL Collector via Tailscale
    otelcol.exporter.otlphttp "k8s" {
      client {
        endpoint = "${cfg.k8sEndpoint}"
        headers = {
          "X-Scope-OrgID" = "anonymous",
        }
      }

      retry_on_failure {
        enabled          = true
        initial_interval = "5s"
        max_interval     = "30s"
        max_elapsed_time = "5m"
      }

      sending_queue {
        enabled       = true
        num_consumers = 10
        queue_size    = 5000
      }
    }

    ${optionalString cfg.langfuse.enable ''
    // =============================================================================
    // Feature 132: Langfuse - AI Observability Platform
    // Export traces to Langfuse for specialized LLM tracing and analytics
    // =============================================================================

    // Langfuse OTEL HTTP Exporter
    // Uses HTTP Basic Auth with base64-encoded public_key:secret_key
    otelcol.exporter.otlphttp "langfuse" {
      client {
        endpoint = "${cfg.langfuse.endpoint}"
        headers = {
          // Authorization header is set via environment variable
          // Format: Basic <base64(public_key:secret_key)>
          "Authorization" = env("LANGFUSE_AUTH_HEADER"),
        }
      }

      ${optionalString cfg.langfuse.retryEnabled ''
      retry_on_failure {
        enabled          = true
        initial_interval = "5s"
        max_interval     = "30s"
        max_elapsed_time = "5m"
      }
      ''}

      sending_queue {
        enabled       = true
        num_consumers = 5
        queue_size    = ${toString cfg.langfuse.queueSize}
      }
    }

    // Langfuse-specific batch processor for optimized trace export
    otelcol.processor.batch "langfuse" {
      send_batch_size     = ${toString cfg.langfuse.batchSize}
      timeout             = "${cfg.langfuse.batchTimeout}"
      send_batch_max_size = ${toString (cfg.langfuse.batchSize * 2)}

      output {
        traces = [otelcol.exporter.otlphttp.langfuse.input]
      }
    }
    ''}

    ${optionalString cfg.enableNodeExporter ''
    // =============================================================================
    // Node Exporter - System metrics (CPU, memory, disk, network)
    // =============================================================================

    prometheus.exporter.unix "default" {
      // Default collectors: cpu, diskstats, filesystem, loadavg, meminfo, netdev, etc.
    }

    prometheus.scrape "node" {
      targets    = prometheus.exporter.unix.default.targets
      forward_to = [prometheus.remote_write.k8s.receiver]

      scrape_interval = "15s"
    }

    prometheus.remote_write "k8s" {
      endpoint {
        url = "${cfg.mimirEndpoint}/api/v1/push"
        // Ensure exemplars (trace IDs) are forwarded when present (e.g. from spanmetrics).
        send_exemplars = true
        headers = {
          "X-Scope-OrgID" = "anonymous",
        }
      }
    }
    ''}

    ${optionalString cfg.enableJournald ''
    // =============================================================================
    // Journald - Log collection from systemd services
    // =============================================================================

    ${journalSourceBlocks}

    // Add hostname to all logs
    loki.relabel "add_host" {
      forward_to = [loki.write.k8s.receiver]

      rule {
        source_labels = ["__journal__hostname"]
        target_label  = "host"
      }

      rule {
        source_labels = ["__journal_priority_keyword"]
        target_label  = "level"
      }
    }

    loki.write "k8s" {
      endpoint {
        url = "${cfg.lokiEndpoint}/loki/api/v1/push"
        headers = {
          "X-Scope-OrgID" = "anonymous",
        }
      }
    }
    ''}
  '';
in
{
  options.services.grafana-alloy = {
    enable = mkEnableOption "Grafana Alloy telemetry collector";

    package = mkOption {
      type = types.package;
      default = pkgs.grafana-alloy;
      description = "Grafana Alloy package to use";
    };

    # Feature 125: Parameterized cluster targeting for multi-cluster observability
    # DEPRECATED: Use cnoe.localtest.me endpoints instead (same for all local clusters)
    targetCluster = mkOption {
      type = types.str;
      default = "";
      example = "ryzen";
      description = ''
        DEPRECATED: No longer used. Endpoints now default to cnoe.localtest.me:8443.
        Kept for backwards compatibility.
      '';
    };

    configFile = mkOption {
      type = types.nullOr types.path;
      default = null;
      description = "Path to custom Alloy configuration file. If null, generates from options.";
    };

    otlpPort = mkOption {
      type = types.port;
      default = 4318;
      description = "OTLP HTTP receiver port (standard: 4318)";
    };

    localForwardPort = mkOption {
      type = types.port;
      default = 4320;
      description = "Port for local otel-ai-monitor forwarding";
    };

    k8sEndpoint = mkOption {
      type = types.str;
      default = "https://otel-collector.cnoe.localtest.me:8443";
      description = "Kubernetes OTEL collector endpoint (via nginx ingress)";
    };

    phoenixEndpoint = mkOption {
      type = types.str;
      default = "http://localhost:6006";
      description = "Arize Phoenix OTLP HTTP endpoint";
    };

    lokiEndpoint = mkOption {
      type = types.str;
      default = "https://loki.cnoe.localtest.me:8443";
      description = "Loki push endpoint (via nginx ingress)";
    };

    mimirEndpoint = mkOption {
      type = types.str;
      default = "https://mimir.cnoe.localtest.me:8443";
      description = "Mimir remote write endpoint (via nginx ingress)";
    };

    enableNodeExporter = mkOption {
      type = types.bool;
      default = true;
      description = "Enable system metrics collection via node exporter";
    };

    enableJournald = mkOption {
      type = types.bool;
      default = true;
      description = "Enable journald log collection";
    };

    journaldUnits = mkOption {
      type = types.listOf types.str;
      default = [
        "otel-ai-monitor.service"
        "i3pm-daemon.service"
        "grafana-alloy.service"
      ];
      description = "Systemd units to collect logs from";
    };

    stopTimeout = mkOption {
      type = types.str;
      default = "10s";
      description = ''
        Maximum time to wait for Alloy to shutdown gracefully.
        Telemetry collectors may try to flush buffered data on shutdown,
        which can delay reboot/shutdown. A shorter timeout (10-30s) is
        recommended since data loss on shutdown is generally acceptable.
      '';
    };

    # Feature 132: Langfuse Integration
    langfuse = {
      enable = mkEnableOption "Langfuse trace export for AI observability";

      endpoint = mkOption {
        type = types.str;
        default = "https://cloud.langfuse.com/api/public/otel";
        example = "https://us.cloud.langfuse.com/api/public/otel";
        description = ''
          Langfuse OTEL HTTP endpoint URL.
          - EU Cloud: https://cloud.langfuse.com/api/public/otel
          - US Cloud: https://us.cloud.langfuse.com/api/public/otel
          - Self-hosted: http://localhost:3000/api/public/otel
        '';
      };

      publicKeyFile = mkOption {
        type = types.nullOr types.path;
        default = null;
        description = ''
          Path to file containing Langfuse public key (pk-lf-...).
          If null, uses LANGFUSE_PUBLIC_KEY environment variable.
        '';
      };

      secretKeyFile = mkOption {
        type = types.nullOr types.path;
        default = null;
        description = ''
          Path to file containing Langfuse secret key (sk-lf-...).
          If null, uses LANGFUSE_SECRET_KEY environment variable.
          IMPORTANT: Use a secrets management solution (sops-nix, agenix)
          to provide this file securely.
        '';
      };

      batchTimeout = mkOption {
        type = types.str;
        default = "10s";
        description = "Batch timeout for Langfuse export (traces sent after this duration)";
      };

      batchSize = mkOption {
        type = types.int;
        default = 100;
        description = "Maximum batch size for Langfuse export";
      };

      retryEnabled = mkOption {
        type = types.bool;
        default = true;
        description = "Enable retry on failure for Langfuse export";
      };

      queueSize = mkOption {
        type = types.int;
        default = 1000;
        description = "Queue size for Langfuse export (traces buffered when endpoint unavailable)";
      };

      credentialSource = mkOption {
        type = types.enum [ "env" "files" "1password" "azure" ];
        default = "env";
        description = ''
          Source for Langfuse credentials:
          - "env": Use LANGFUSE_PUBLIC_KEY and LANGFUSE_SECRET_KEY environment variables
          - "files": Use publicKeyFile and secretKeyFile paths
          - "1password": Use 1Password CLI to fetch keys
          - "azure": Use Azure Key Vault to fetch keys
        '';
      };

      azureKeyVaultName = mkOption {
        type = types.nullOr types.str;
        default = null;
        description = ''
          Azure Key Vault name (required when credentialSource = "azure").
          Secrets expected: LANGFUSE-PUBLIC-KEY and LANGFUSE-SECRET-KEY.
        '';
      };

      onePasswordRefs = mkOption {
        type = types.nullOr (types.attrsOf types.str);
        default = null;
        example = {
          publicKey = "op://Development/Langfuse/public_key";
          secretKey = "op://Development/Langfuse/secret_key";
        };
        description = ''
          1Password references for Langfuse keys (required when credentialSource = "1password").
        '';
      };

      environmentFile = mkOption {
        type = types.nullOr types.path;
        default = null;
        example = "/run/secrets/langfuse-env";
        description = ''
          Path to environment file containing LANGFUSE_PUBLIC_KEY and LANGFUSE_SECRET_KEY.
          This is used as a fallback when 1Password or Azure Key Vault are unavailable.
          File format: KEY=value (one per line, no quotes needed).
          This file is loaded by systemd before the service starts.
        '';
      };
    };
  };

  config = mkIf cfg.enable {
    # Endpoints now use cnoe.localtest.me:8443 by default (same for all local clusters)
    # Can be overridden via explicit config if needed

    # Ensure package is available
    environment.systemPackages = [ cfg.package ];

    # Install configuration file
    environment.etc."alloy/config.alloy".source =
      if cfg.configFile != null then cfg.configFile else alloyConfig;

    # Systemd service for Grafana Alloy
    systemd.services.grafana-alloy = {
      description = "Grafana Alloy - OpenTelemetry Collector";
      documentation = [ "https://grafana.com/docs/alloy/" ];

      wantedBy = [ "multi-user.target" ];
      after = [ "network-online.target" ];
      wants = [ "network-online.target" ];

      serviceConfig = let
        # Feature 132: Generate Langfuse auth script based on credential source
        langfuseAuthScript = pkgs.writeShellScript "alloy-with-langfuse" ''
          set -euo pipefail

          # Generate Langfuse auth header based on configured credential source
          ${if cfg.langfuse.credentialSource == "env" then ''
            # Source: environment variables
            if [ -n "''${LANGFUSE_PUBLIC_KEY:-}" ] && [ -n "''${LANGFUSE_SECRET_KEY:-}" ]; then
              export LANGFUSE_AUTH_HEADER="Basic $(echo -n "$LANGFUSE_PUBLIC_KEY:$LANGFUSE_SECRET_KEY" | ${pkgs.coreutils}/bin/base64 -w0)"
            else
              echo "WARNING: LANGFUSE_PUBLIC_KEY and LANGFUSE_SECRET_KEY not set. Trace export will fail." >&2
              export LANGFUSE_AUTH_HEADER=""
            fi
          '' else if cfg.langfuse.credentialSource == "files" then ''
            # Source: key files
            ${if cfg.langfuse.publicKeyFile != null && cfg.langfuse.secretKeyFile != null then ''
              PUBLIC_KEY=$(${pkgs.coreutils}/bin/cat "${toString cfg.langfuse.publicKeyFile}" | ${pkgs.coreutils}/bin/tr -d '[:space:]')
              SECRET_KEY=$(${pkgs.coreutils}/bin/cat "${toString cfg.langfuse.secretKeyFile}" | ${pkgs.coreutils}/bin/tr -d '[:space:]')
              export LANGFUSE_AUTH_HEADER="Basic $(echo -n "$PUBLIC_KEY:$SECRET_KEY" | ${pkgs.coreutils}/bin/base64 -w0)"
            '' else ''
              echo "ERROR: credentialSource=files but publicKeyFile/secretKeyFile not configured" >&2
              export LANGFUSE_AUTH_HEADER=""
            ''}
          '' else if cfg.langfuse.credentialSource == "1password" then ''
            # Source: 1Password CLI
            # Note: 1Password CLI requires user session, which may not be available in system services
            if ! command -v ${pkgs._1password-cli}/bin/op &> /dev/null; then
              echo "WARN: 1Password CLI not found, Langfuse disabled" >&2
              export LANGFUSE_AUTH_HEADER=""
            else
              ${if cfg.langfuse.onePasswordRefs != null then ''
                # Try to fetch credentials, but don't fail if 1Password isn't available
                # (e.g., when running as system service without user session)
                PUBLIC_KEY=""
                SECRET_KEY=""
                if PUBLIC_KEY=$(${pkgs._1password-cli}/bin/op read "${cfg.langfuse.onePasswordRefs.publicKey or "op://Development/Langfuse/public_key"}" 2>/dev/null); then
                  if SECRET_KEY=$(${pkgs._1password-cli}/bin/op read "${cfg.langfuse.onePasswordRefs.secretKey or "op://Development/Langfuse/secret_key"}" 2>/dev/null); then
                    export LANGFUSE_AUTH_HEADER="Basic $(echo -n "$PUBLIC_KEY:$SECRET_KEY" | ${pkgs.coreutils}/bin/base64 -w0)"
                    echo "INFO: Langfuse credentials loaded from 1Password" >&2
                  else
                    echo "WARN: Failed to read secret_key from 1Password, Langfuse disabled" >&2
                    export LANGFUSE_AUTH_HEADER=""
                  fi
                else
                  # 1Password not available, try environment variables as fallback
                  if [ -n "''${LANGFUSE_PUBLIC_KEY:-}" ] && [ -n "''${LANGFUSE_SECRET_KEY:-}" ]; then
                    echo "INFO: 1Password not available, using LANGFUSE_PUBLIC_KEY/LANGFUSE_SECRET_KEY env vars" >&2
                    export LANGFUSE_AUTH_HEADER="Basic $(echo -n "$LANGFUSE_PUBLIC_KEY:$LANGFUSE_SECRET_KEY" | ${pkgs.coreutils}/bin/base64 -w0)"
                  else
                    echo "WARN: 1Password session not available (expected for system services)" >&2
                    echo "WARN: Set LANGFUSE_PUBLIC_KEY and LANGFUSE_SECRET_KEY env vars, or use credentialSource=files" >&2
                    export LANGFUSE_AUTH_HEADER=""
                  fi
                fi
              '' else ''
                echo "WARN: credentialSource=1password but onePasswordRefs not configured, Langfuse disabled" >&2
                export LANGFUSE_AUTH_HEADER=""
              ''}
            fi
          '' else if cfg.langfuse.credentialSource == "azure" then ''
            # Source: Azure Key Vault
            if ! command -v ${pkgs.azure-cli}/bin/az &> /dev/null; then
              echo "ERROR: Azure CLI not found" >&2
              export LANGFUSE_AUTH_HEADER=""
            elif [ -z "${cfg.langfuse.azureKeyVaultName or ""}" ]; then
              echo "ERROR: credentialSource=azure but azureKeyVaultName not configured" >&2
              export LANGFUSE_AUTH_HEADER=""
            else
              PUBLIC_KEY=$(${pkgs.azure-cli}/bin/az keyvault secret show \
                --vault-name "${cfg.langfuse.azureKeyVaultName}" \
                --name "LANGFUSE-PUBLIC-KEY" \
                --query "value" -o tsv)
              SECRET_KEY=$(${pkgs.azure-cli}/bin/az keyvault secret show \
                --vault-name "${cfg.langfuse.azureKeyVaultName}" \
                --name "LANGFUSE-SECRET-KEY" \
                --query "value" -o tsv)
              if [ -n "$PUBLIC_KEY" ] && [ -n "$SECRET_KEY" ]; then
                export LANGFUSE_AUTH_HEADER="Basic $(echo -n "$PUBLIC_KEY:$SECRET_KEY" | ${pkgs.coreutils}/bin/base64 -w0)"
              else
                echo "ERROR: Failed to fetch keys from Azure Key Vault" >&2
                export LANGFUSE_AUTH_HEADER=""
              fi
            fi
          '' else ''
            echo "ERROR: Unknown credential source" >&2
            export LANGFUSE_AUTH_HEADER=""
          ''}

          exec ${cfg.package}/bin/alloy run /etc/alloy/config.alloy
        '';
      in {
        Type = "simple";
        ExecStart =
          if cfg.langfuse.enable then
            "${langfuseAuthScript}"
          else
            "${cfg.package}/bin/alloy run /etc/alloy/config.alloy";
        Restart = "always";
        RestartSec = "5s";

        # Limit shutdown wait time - telemetry data loss on reboot is acceptable
        TimeoutStopSec = cfg.stopTimeout;

        # Feature 132: Load environment file for Langfuse credentials if configured
        EnvironmentFile = lib.mkIf (cfg.langfuse.enable && cfg.langfuse.environmentFile != null)
          [ (toString cfg.langfuse.environmentFile) ];

        # Security hardening
        DynamicUser = true;
        ProtectSystem = "strict";
        ProtectHome = true;
        PrivateTmp = true;
        NoNewPrivileges = true;

        # Need read access to journald
        SupplementaryGroups = [ "systemd-journal" ];
        ReadOnlyPaths = [ "/var/log/journal" ]
          # Feature 132: Read access to Langfuse key files if configured
          ++ lib.optionals (cfg.langfuse.enable && cfg.langfuse.publicKeyFile != null)
            [ (toString cfg.langfuse.publicKeyFile) ]
          ++ lib.optionals (cfg.langfuse.enable && cfg.langfuse.secretKeyFile != null)
            [ (toString cfg.langfuse.secretKeyFile) ];

        # Working directory for any state
        StateDirectory = "grafana-alloy";
        WorkingDirectory = "/var/lib/grafana-alloy";

        # Memory limit per spec (200MB)
        MemoryMax = "200M";
      };
    };

    # Open firewall for OTLP port (only on localhost by default)
    # If you need external access, configure firewall separately
    networking.firewall.interfaces."tailscale0".allowedTCPPorts = [ cfg.otlpPort ];
  };
}
