# Feature 129: Grafana Alloy - Unified OpenTelemetry Collector
#
# This module provides Grafana Alloy as the unified telemetry collector that:
# - Replaces otel-ai-collector.nix with a more capable solution
# - Receives OTLP telemetry from AI CLIs (Claude Code, Codex, Gemini) on port 4318
# - Forwards to otel-ai-monitor user service on port 4320 for local session tracking
# - Exports all telemetry to Kubernetes LGTM stack via Tailscale
# - Collects system metrics via built-in node exporter
# - Collects journald logs and forwards to Loki
#
# Architecture:
#   AI CLIs → Alloy :4318 → [batch processor] → otel-ai-monitor :4320 (local)
#                                            → K8s OTEL Collector (remote)
#   System → node exporter → Alloy → Mimir (K8s)
#   Journald → Alloy → Loki (K8s)
#
{ config, lib, pkgs, ... }:

with lib;

let
  cfg = config.services.grafana-alloy;

  # Generate journal source blocks for each configured unit
  journalSourceBlocks = concatMapStringsSep "\n\n" (unit:
    let
      # Convert unit name to valid identifier (replace dots and hyphens)
      safeName = replaceStrings ["-" "."] ["_" "_"] (removeSuffix ".service" unit);
      serviceName = removeSuffix ".service" unit;
    in ''
      loki.source.journal "${safeName}" {
        path    = "/var/log/journal"
        matches = "_SYSTEMD_UNIT=${unit}"

        labels = {
          job     = "systemd-journal",
          service = "${serviceName}",
        }

        forward_to = [loki.relabel.add_host.receiver]
      }''
  ) cfg.journaldUnits;

  # Generate the complete Alloy configuration
  alloyConfig = pkgs.writeText "alloy-config.alloy" ''
    // Grafana Alloy Configuration
    // Generated by NixOS module: Feature 129
    // DO NOT EDIT - managed by nixos-rebuild

    // =============================================================================
    // Live Debugging - Enable debug UI at http://localhost:12345
    // =============================================================================

    livedebugging {
      enabled = true
    }

    // =============================================================================
    // OTLP Receiver - Receives telemetry from AI CLIs and other apps
    // =============================================================================

    otelcol.receiver.otlp "default" {
      http {
        endpoint = "0.0.0.0:${toString cfg.otlpPort}"
      }

      output {
        metrics = [otelcol.processor.batch.default.input]
        logs    = [otelcol.processor.batch.default.input]
        traces  = [otelcol.processor.batch.default.input]
      }
    }

    // =============================================================================
    // Batch Processor - Buffer and batch telemetry for efficiency
    // =============================================================================

    otelcol.processor.batch "default" {
      send_batch_size     = 1000
      timeout             = "10s"
      send_batch_max_size = 2000  // Memory limit ~100MB worth of telemetry

      output {
        metrics = [
          otelcol.exporter.otlphttp.local.input,
          otelcol.exporter.otlphttp.k8s.input,
        ]
        logs = [
          otelcol.exporter.otlphttp.local.input,
          otelcol.exporter.otlphttp.k8s.input,
        ]
        traces = [
          otelcol.exporter.otlphttp.local.input,
          otelcol.exporter.otlphttp.k8s.input,
        ]
      }
    }

    // =============================================================================
    // Exporters - Send to local otel-ai-monitor and remote K8s
    // =============================================================================

    // Local: otel-ai-monitor for EWW widgets
    otelcol.exporter.otlphttp "local" {
      client {
        endpoint = "http://localhost:${toString cfg.localForwardPort}"
        tls {
          insecure = true
        }
      }
    }

    // Remote: Kubernetes OTEL Collector via Tailscale
    otelcol.exporter.otlphttp "k8s" {
      client {
        endpoint = "${cfg.k8sEndpoint}"
      }

      retry_on_failure {
        enabled          = true
        initial_interval = "5s"
        max_interval     = "30s"
        max_elapsed_time = "5m"
      }

      sending_queue {
        enabled       = true
        num_consumers = 10
        queue_size    = 5000
      }
    }

    ${optionalString cfg.enableNodeExporter ''
    // =============================================================================
    // Node Exporter - System metrics (CPU, memory, disk, network)
    // =============================================================================

    prometheus.exporter.unix "default" {
      // Default collectors: cpu, diskstats, filesystem, loadavg, meminfo, netdev, etc.
    }

    prometheus.scrape "node" {
      targets    = prometheus.exporter.unix.default.targets
      forward_to = [prometheus.remote_write.k8s.receiver]

      scrape_interval = "15s"
    }

    prometheus.remote_write "k8s" {
      endpoint {
        url = "${cfg.mimirEndpoint}/api/v1/push"
      }
    }
    ''}

    ${optionalString cfg.enableJournald ''
    // =============================================================================
    // Journald - Log collection from systemd services
    // =============================================================================

    ${journalSourceBlocks}

    // Add hostname to all logs
    loki.relabel "add_host" {
      forward_to = [loki.write.k8s.receiver]

      rule {
        source_labels = ["__journal__hostname"]
        target_label  = "host"
      }

      rule {
        source_labels = ["__journal_priority_keyword"]
        target_label  = "level"
      }
    }

    loki.write "k8s" {
      endpoint {
        url = "${cfg.lokiEndpoint}/loki/api/v1/push"
      }
    }
    ''}
  '';
in
{
  options.services.grafana-alloy = {
    enable = mkEnableOption "Grafana Alloy telemetry collector";

    package = mkOption {
      type = types.package;
      default = pkgs.grafana-alloy;
      description = "Grafana Alloy package to use";
    };

    configFile = mkOption {
      type = types.nullOr types.path;
      default = null;
      description = "Path to custom Alloy configuration file. If null, generates from options.";
    };

    otlpPort = mkOption {
      type = types.port;
      default = 4318;
      description = "OTLP HTTP receiver port (standard: 4318)";
    };

    localForwardPort = mkOption {
      type = types.port;
      default = 4320;
      description = "Port for local otel-ai-monitor forwarding";
    };

    k8sEndpoint = mkOption {
      type = types.str;
      default = "https://otel-collector-1.tail286401.ts.net";
      description = "Kubernetes OTEL collector endpoint (via Tailscale Operator Ingress, HTTPS:443)";
    };

    lokiEndpoint = mkOption {
      type = types.str;
      default = "https://loki.tail286401.ts.net";
      description = "Loki push endpoint (via Tailscale Serve, HTTPS:443)";
    };

    mimirEndpoint = mkOption {
      type = types.str;
      default = "https://mimir.tail286401.ts.net";
      description = "Mimir remote write endpoint (via Tailscale Serve, HTTPS:443)";
    };

    enableNodeExporter = mkOption {
      type = types.bool;
      default = true;
      description = "Enable system metrics collection via node exporter";
    };

    enableJournald = mkOption {
      type = types.bool;
      default = true;
      description = "Enable journald log collection";
    };

    journaldUnits = mkOption {
      type = types.listOf types.str;
      default = [
        "otel-ai-monitor.service"
        "i3pm-daemon.service"
        "grafana-alloy.service"
      ];
      description = "Systemd units to collect logs from";
    };
  };

  config = mkIf cfg.enable {
    # Ensure package is available
    environment.systemPackages = [ cfg.package ];

    # Install configuration file
    environment.etc."alloy/config.alloy".source =
      if cfg.configFile != null then cfg.configFile else alloyConfig;

    # Systemd service for Grafana Alloy
    systemd.services.grafana-alloy = {
      description = "Grafana Alloy - OpenTelemetry Collector";
      documentation = [ "https://grafana.com/docs/alloy/" ];

      wantedBy = [ "multi-user.target" ];
      after = [ "network-online.target" ];
      wants = [ "network-online.target" ];

      serviceConfig = {
        Type = "simple";
        ExecStart = "${cfg.package}/bin/alloy run /etc/alloy/config.alloy";
        Restart = "always";
        RestartSec = "5s";

        # Security hardening
        DynamicUser = true;
        ProtectSystem = "strict";
        ProtectHome = true;
        PrivateTmp = true;
        NoNewPrivileges = true;

        # Need read access to journald
        SupplementaryGroups = [ "systemd-journal" ];
        ReadOnlyPaths = [ "/var/log/journal" ];

        # Working directory for any state
        StateDirectory = "grafana-alloy";
        WorkingDirectory = "/var/lib/grafana-alloy";

        # Memory limit per spec (200MB)
        MemoryMax = "200M";
      };
    };

    # Open firewall for OTLP port (only on localhost by default)
    # If you need external access, configure firewall separately
    networking.firewall.interfaces."tailscale0".allowedTCPPorts = [ cfg.otlpPort ];
  };
}
