# OpenTelemetry Semantic Conventions for Generative AI

Consolidated documentation for GenAI spans, events, metrics, and technology-specific conventions (OpenAI, Azure AI Inference, AWS Bedrock).

---

# Semantic conventions for generative client AI spans | OpenTelemetry

Source: https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-spans/

Semantic conventions for generative client AI spans Status: Development Warning Existing GenAI instrumentations that are using v1.36.0 of this document (or prior): SHOULD NOT change the version of the GenAI conventions that they emit by default. Conventions include, but are not limited to, attributes, metric, span and event names, span kind and unit of measure. SHOULD introduce an environment variable OTEL_SEMCONV_STABILITY_OPT_IN as a comma-separated list of category-specific values. The list of values includes: gen_ai_latest_experimental - emit the latest experimental version of GenAI conventions (supported by the instrumentation) and do not emit the old one (v1.36.0 or prior). The default behavior is to continue emitting whatever version of the GenAI conventions the instrumentation was emitting (1.36.0 or prior). This transition plan will be updated to include stable version before the GenAI conventions are marked as stable. Spans Inference Status: Development This span represents a client call to Generative AI model or service that generates a response or requests a tool call based on the input prompt. Span name SHOULD be {gen_ai.operation.name} {gen_ai.request.model} . Semantic conventions for individual GenAI systems and frameworks MAY specify different span name format and MUST follow the overall guidelines for span names. Span kind SHOULD be CLIENT and MAY be set to INTERNAL on spans representing call to models running in the same process. It's RECOMMENDED to use CLIENT kind when the GenAI system being instrumented usually runs in a different process than its client or when the GenAI call happens over instrumented protocol such as HTTP. Span status SHOULD follow the Recording Errors document. Attributes: Key Stability Requirement Level Value Type Description Example Values gen_ai.operation.name Development Required string The name of the operation being performed. [1] chat ; generate_content ; text_completion gen_ai.provider.name Development Required string The Generative AI provider as identified by the client or server instrumentation. [2] openai ; gcp.gen_ai ; gcp.vertex_ai error.type Stable Conditionally Required if the operation ended in an error string Describes a class of error the operation ended with. [3] timeout ; java.net.UnknownHostException ; server_certificate_invalid ; 500 gen_ai.conversation.id Development Conditionally Required when available string The unique identifier for a conversation (session, thread), used to store and correlate messages within this conversation. [4] conv_5j66UpCpwteGg4YSxUnt7lPY gen_ai.output.type Development Conditionally Required [5] string Represents the content type requested by the client. [6] text ; json ; image gen_ai.request.choice.count Development Conditionally Required if available, in the request, and !=1 int The target number of candidate completions to return. 3 gen_ai.request.model Development Conditionally Required If available. string The name of the GenAI model a request is being made to. [7] gpt-4 gen_ai.request.seed Development Conditionally Required if applicable and if the request includes a seed int Requests with same seed value more likely to return same result. 100 server.port Stable Conditionally Required If server.address is set. int GenAI server port. [8] 80 ; 8080 ; 443 gen_ai.request.frequency_penalty Development Recommended double The frequency penalty setting for the GenAI request. 0.1 gen_ai.request.max_tokens Development Recommended int The maximum number of tokens the model generates for a request. 100 gen_ai.request.presence_penalty Development Recommended double The presence penalty setting for the GenAI request. 0.1 gen_ai.request.stop_sequences Development Recommended string[] List of sequences that the model will use to stop generating further tokens. ["forest", "lived"] gen_ai.request.temperature Development Recommended double The temperature setting for the GenAI request. 0.0 gen_ai.request.top_k Development Recommended double The top_k sampling setting for the GenAI request. 1.0 gen_ai.request.top_p Development Recommended double The top_p sampling setting for the GenAI request. 1.0 gen_ai.response.finish_reasons Development Recommended string[] Array of reasons the model stopped generating tokens, corresponding to each generation received. ["stop"] ; ["stop", "length"] gen_ai.response.id Development Recommended string The unique identifier for the completion. chatcmpl-123 gen_ai.response.model Development Recommended string The name of the model that generated the response. [9] gpt-4-0613 gen_ai.usage.input_tokens Development Recommended int The number of tokens used in the GenAI input (prompt). 100 gen_ai.usage.output_tokens Development Recommended int The number of tokens used in the GenAI response (completion). 180 server.address Stable Recommended string GenAI server address. [10] example.com ; 10.1.2.80 ; /tmp/my.sock gen_ai.input.messages Development Opt-In any The chat history provided to the model as an input. [11] [ { “role”: “user”, “parts”: [ { “type”: “text”, “content”: “Weather in Paris?" } ] }, { “role”: “assistant”, “parts”: [ { “type”: “tool_call”, “id”: “call_VSPygqKTWdrhaFErNvMV18Yl”, “name”: “get_weather”, “arguments”: { “location”: “Paris” } } ] }, { “role”: “tool”, “parts”: [ { “type”: “tool_call_response”, “id”: " call_VSPygqKTWdrhaFErNvMV18Yl”, “result”: “rainy, 57°F” } ] } ] gen_ai.output.messages Development Opt-In any Messages returned by the model where each message represents a specific model response (choice, candidate). [12] [ { “role”: “assistant”, “parts”: [ { “type”: “text”, “content”: “The weather in Paris is currently rainy with a temperature of 57°F." } ], “finish_reason”: “stop” } ] gen_ai.system_instructions Development Opt-In any The system message or instructions provided to the GenAI model separately from the chat history. [13] [ { “type”: “text”, “content”: “You are an Agent that greet users, always use greetings tool to respond” } ]; [ { “type”: “text”, “content”: “You are a language translator." }, { “type”: “text”, “content”: “Your mission is to translate text in English to French." } ] gen_ai.tool.definitions Development Opt-In any The list of source system tool definitions available to the GenAI agent or model. [14] [ { “type”: “function”, “name”: “get_current_weather”, “description”: “Get the current weather in a given location”, “parameters”: { “type”: “object”, “properties”: { “location”: { “type”: “string”, “description”: “The city and state, e.g. San Francisco, CA” }, “unit”: { “type”: “string”, “enum”: [ “celsius”, “fahrenheit” ] } } }, “required”: [ “location”, “unit” ] } } ] [1] gen_ai.operation.name : If one of the predefined values applies, but specific system uses a different name it's RECOMMENDED to document it in the semantic conventions for specific GenAI system and use system-specific name in the instrumentation. If a different name is not documented, instrumentation libraries SHOULD use applicable predefined value. [2] gen_ai.provider.name : The attribute SHOULD be set based on the instrumentation's best knowledge and may differ from the actual model provider. Multiple providers, including Azure OpenAI, Gemini, and AI hosting platforms are accessible using the OpenAI REST API and corresponding client libraries, but may proxy or host models from different providers. The gen_ai.request.model , gen_ai.response.model , and server.address attributes may help identify the actual system in use. The gen_ai.provider.name attribute acts as a discriminator that identifies the GenAI telemetry format flavor specific to that provider within GenAI semantic conventions. It SHOULD be set consistently with provider-specific attributes and signals. For example, GenAI spans, metrics, and events related to AWS Bedrock should have the gen_ai.provider.name set to aws.bedrock and include applicable aws.bedrock.* attributes and are not expected to include openai.* attributes. [3] error.type : The error.type SHOULD match the error code returned by the Generative AI provider or the client library, the canonical name of exception that occurred, or another low-cardinality error identifier. Instrumentations SHOULD document the list of errors they report. [4] gen_ai.conversation.id : Instrumentations SHOULD populate conversation id when they have it readily available for a given operation, for example: when client framework being instrumented manages conversation history (see LlamaIndex chat store) when instrumenting GenAI client libraries that maintain conversation on the backend side (see AWS Bedrock agent sessions, OpenAI Assistant threads) Application developers that manage conversation history MAY add conversation id to GenAI and other spans or logs using custom span or log record processors or hooks provided by instrumentation libraries. [5] gen_ai.output.type : when applicable and if the request includes an output format. [6] gen_ai.output.type : This attribute SHOULD be used when the client requests output of a specific type. The model may return zero or more outputs of this type. This attribute specifies the output modality and not the actual output format. For example, if an image is requested, the actual output could be a URL pointing to an image file. Additional output format details may be recorded in the future in the gen_ai.output.{type}.* attributes. [7] gen_ai.request.model : The name of the GenAI model a request is being made to. If the model is supplied by a vendor, then the value must be the exact name of the model requested. If the model is a fine-tuned custom model, the value should have a more specific name than the base model that's been fine-tuned. [8] server.port : When observed from the client side, and when communicating through an intermediary, server.port SHOULD represent the server port behind any intermediaries, for example proxies, if it's available. [9] gen_ai.response.model : If available. The name of the GenAI model that provided the response. If the model is supplied by a vendor, then the value must be the exact name of the model actually used. If the model is a fine-tuned custom model, the value should have a more specific name than the base model that's been fine-tuned. [10] server.address : When observed from the client side, and when communicating through an intermediary, server.address SHOULD represent the server address behind any intermediaries, for example proxies, if it's available. [11] gen_ai.input.messages : Instrumentations MUST follow Input messages JSON schema. When the attribute is recorded on events, it MUST be recorded in structured form. When recorded on spans, it MAY be recorded as a JSON string if structured format is not supported and SHOULD be recorded in structured form otherwise. Messages MUST be provided in the order they were sent to the model. Instrumentations MAY provide a way for users to filter or truncate input messages. Warning This attribute is likely to contain sensitive information including user/PII data. See Recording content on attributes section for more details. [12] gen_ai.output.messages : Instrumentations MUST follow Output messages JSON schema Each message represents a single output choice/candidate generated by the model. Each message corresponds to exactly one generation (choice/candidate) and vice versa - one choice cannot be split across multiple messages or one message cannot contain parts from multiple choices. When the attribute is recorded on events, it MUST be recorded in structured form. When recorded on spans, it MAY be recorded as a JSON string if structured format is not supported and SHOULD be recorded in structured form otherwise. Instrumentations MAY provide a way for users to filter or truncate output messages. Warning This attribute is likely to contain sensitive information including user/PII data. See Recording content on attributes section for more details. [13] gen_ai.system_instructions : This attribute SHOULD be used when the corresponding provider or API allows to provide system instructions or messages separately from the chat history. Instructions that are part of the chat history SHOULD be recorded in gen_ai.input.messages attribute instead. Instrumentations MUST follow System instructions JSON schema. When recorded on spans, it MAY be recorded as a JSON string if structured format is not supported and SHOULD be recorded in structured form otherwise. Instrumentations MAY provide a way for users to filter or truncate system instructions. Warning This attribute may contain sensitive information. See Recording content on attributes section for more details. [14] gen_ai.tool.definitions : The value of this attribute matches source system tool definition format. It's expected to be an array of objects where each object represents a tool definition. In case a serialized string is available to the instrumentation, the instrumentation SHOULD do the best effort to deserialize it to an array. When recorded on spans, it MAY be recorded as a JSON string if structured format is not supported and SHOULD be recorded in structured form otherwise. Since this attribute could be large, it's NOT RECOMMENDED to populate it by default. Instrumentations MAY provide a way to enable populating this attribute. error.type has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability _OTHER A fallback error value to be used when the instrumentation doesn't define a custom value. Stable gen_ai.operation.name has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability chat Chat completion operation such as OpenAI Chat API Development create_agent Create GenAI agent Development embeddings Embeddings operation such as OpenAI Create embeddings API Development execute_tool Execute a tool Development generate_content Multimodal content generation operation such as Gemini Generate Content Development invoke_agent Invoke GenAI agent Development text_completion Text completions operation such as OpenAI Completions API (Legacy) Development gen_ai.output.type has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability image Image Development json JSON object with known or unknown schema Development speech Speech Development text Plain text Development Embeddings Status: Development Describes GenAI embeddings span - a request to a Generative AI model or service that generates an embeddings based on the input. The gen_ai.operation.name SHOULD be embeddings . Span name SHOULD be {gen_ai.operation.name} {gen_ai.request.model} . Span kind SHOULD be CLIENT . Span status SHOULD follow the Recording Errors document. Attributes: Key Stability Requirement Level Value Type Description Example Values gen_ai.operation.name Development Required string The name of the operation being performed. [1] chat ; generate_content ; text_completion error.type Stable Conditionally Required if the operation ended in an error string Describes a class of error the operation ended with. [2] timeout ; java.net.UnknownHostException ; server_certificate_invalid ; 500 gen_ai.request.model Development Conditionally Required If available. string The name of the GenAI model a request is being made to. [3] gpt-4 server.port Stable Conditionally Required If server.address is set. int GenAI server port. [4] 80 ; 8080 ; 443 gen_ai.embeddings.dimension.count Development Recommended int The number of dimensions the resulting output embeddings should have. 512 ; 1024 gen_ai.request.encoding_formats Development Recommended string[] The encoding formats requested in an embeddings operation, if specified. [5] ["base64"] ; ["float", "binary"] gen_ai.usage.input_tokens Development Recommended int The number of tokens used in the GenAI input (prompt). 100 server.address Stable Recommended string GenAI server address. [6] example.com ; 10.1.2.80 ; /tmp/my.sock [1] gen_ai.operation.name : If one of the predefined values applies, but specific system uses a different name it's RECOMMENDED to document it in the semantic conventions for specific GenAI system and use system-specific name in the instrumentation. If a different name is not documented, instrumentation libraries SHOULD use applicable predefined value. [2] error.type : The error.type SHOULD match the error code returned by the Generative AI provider or the client library, the canonical name of exception that occurred, or another low-cardinality error identifier. Instrumentations SHOULD document the list of errors they report. [3] gen_ai.request.model : The name of the GenAI model a request is being made to. If the model is supplied by a vendor, then the value must be the exact name of the model requested. If the model is a fine-tuned custom model, the value should have a more specific name than the base model that's been fine-tuned. [4] server.port : When observed from the client side, and when communicating through an intermediary, server.port SHOULD represent the server port behind any intermediaries, for example proxies, if it's available. [5] gen_ai.request.encoding_formats : In some GenAI systems the encoding formats are called embedding types. Also, some GenAI systems only accept a single format per request. [6] server.address : When observed from the client side, and when communicating through an intermediary, server.address SHOULD represent the server address behind any intermediaries, for example proxies, if it's available. error.type has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability _OTHER A fallback error value to be used when the instrumentation doesn't define a custom value. Stable gen_ai.operation.name has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability chat Chat completion operation such as OpenAI Chat API Development create_agent Create GenAI agent Development embeddings Embeddings operation such as OpenAI Create embeddings API Development execute_tool Execute a tool Development generate_content Multimodal content generation operation such as Gemini Generate Content Development invoke_agent Invoke GenAI agent Development text_completion Text completions operation such as OpenAI Completions API (Legacy) Development Execute tool span Status: Development Describes tool execution span. gen_ai.operation.name SHOULD be execute_tool . Span name SHOULD be execute_tool {gen_ai.tool.name} . GenAI instrumentations that are able to instrument tool execution call SHOULD do so. However, it's common for tools to be executed by the application code. It's recommended for the application developers to follow this semantic convention for tools invoked by the application code. Span kind SHOULD be INTERNAL . Span status SHOULD follow the Recording Errors document. Attributes: Key Stability Requirement Level Value Type Description Example Values gen_ai.operation.name Development Required string The name of the operation being performed. [1] chat ; generate_content ; text_completion error.type Stable Conditionally Required if the operation ended in an error string Describes a class of error the operation ended with. [2] timeout ; java.net.UnknownHostException ; server_certificate_invalid ; 500 gen_ai.tool.call.id Development Recommended if available string The tool call identifier. call_mszuSIzqtI65i1wAUOE8w5H4 gen_ai.tool.description Development Recommended if available string The tool description. Multiply two numbers gen_ai.tool.name Development Recommended string Name of the tool utilized by the agent. Flights gen_ai.tool.type Development Recommended if available string Type of the tool utilized by the agent [3] function ; extension ; datastore gen_ai.tool.call.arguments Development Opt-In any Parameters passed to the tool call. [4] { “location”: “San Francisco?”, “date”: “2025-10-01” } gen_ai.tool.call.result Development Opt-In any The result returned by the tool call (if any and if execution was successful). [5] { “temperature_range”: { “high”: 75, “low”: 60 }, “conditions”: “sunny” } [1] gen_ai.operation.name : If one of the predefined values applies, but specific system uses a different name it's RECOMMENDED to document it in the semantic conventions for specific GenAI system and use system-specific name in the instrumentation. If a different name is not documented, instrumentation libraries SHOULD use applicable predefined value. [2] error.type : The error.type SHOULD match the error code returned by the Generative AI provider or the client library, the canonical name of exception that occurred, or another low-cardinality error identifier. Instrumentations SHOULD document the list of errors they report. [3] gen_ai.tool.type : Extension: A tool executed on the agent-side to directly call external APIs, bridging the gap between the agent and real-world systems. Agent-side operations involve actions that are performed by the agent on the server or within the agent's controlled environment. Function: A tool executed on the client-side, where the agent generates parameters for a predefined function, and the client executes the logic. Client-side operations are actions taken on the user's end or within the client application. Datastore: A tool used by the agent to access and query structured or unstructured external data for retrieval-augmented tasks or knowledge updates. [4] gen_ai.tool.call.arguments : > [!WARNING] This attribute may contain sensitive information. It's expected to be an object - in case a serialized string is available to the instrumentation, the instrumentation SHOULD do the best effort to deserialize it to an object. When recorded on spans, it MAY be recorded as a JSON string if structured format is not supported and SHOULD be recorded in structured form otherwise. [5] gen_ai.tool.call.result : > [!WARNING] This attribute may contain sensitive information. It's expected to be an object - in case a serialized string is available to the instrumentation, the instrumentation SHOULD do the best effort to deserialize it to an object. When recorded on spans, it MAY be recorded as a JSON string if structured format is not supported and SHOULD be recorded in structured form otherwise. error.type has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability _OTHER A fallback error value to be used when the instrumentation doesn't define a custom value. Stable gen_ai.operation.name has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability chat Chat completion operation such as OpenAI Chat API Development create_agent Create GenAI agent Development embeddings Embeddings operation such as OpenAI Create embeddings API Development execute_tool Execute a tool Development generate_content Multimodal content generation operation such as Gemini Generate Content Development invoke_agent Invoke GenAI agent Development text_completion Text completions operation such as OpenAI Completions API (Legacy) Development Capturing instructions, inputs, and outputs Full (buffered) content Model instructions, user messages, and model outputs are considered sensitive and are often large in size. Recording large or sensitive content in telemetry may be problematic due to high storage costs, regulatory requirements, or the need to enforce different access models for operational and user data. OpenTelemetry instrumentations SHOULD NOT capture them by default, but SHOULD provide an option for users to opt in. Application developers should choose an appropriate usage pattern based on application needs and maturity: [Default] Don't record instructions, inputs, or outputs. Record instructions, inputs, and outputs on the GenAI spans using corresponding attributes ( gen_ai.system_instructions , gen_ai.input.messages , gen_ai.output.messages ). This approach is best suited for situations where telemetry volume is manageable and either privacy regulations do not apply or the telemetry storage complies with them, for example, in pre-production environments. See Recording content on attributes section for more details. Store content externally and record references on the spans. This pattern is recommended in production environments where telemetry volume is a concern or sensitive data needs to be handled securely. Using external storage enables separate access controls. See Uploading content to external storage section for more details. Recording content on attributes The content captured in gen_ai.system_instructions , gen_ai.input.messages , and gen_ai.output.messages attributes is likely to be large. It may contain media, and even in the text form, it may be larger than observability backend limits for telemetry envelopes or attribute values. The inputs and outputs attributes follow common structure formally defined in inputs JSON schema and outputs JSON schema. See also their representation in Python code. Note Recording structured attributes is supported on events (or logs) and may not yet be supported on spans. See OTEP: Extending attributes to support complex values for the details. If structured attributes are not yet supported on spans in a given language, the corresponding attribute value SHOULD be serialized to JSON string on spans and recorded in its structured form on events. Instrumentation MAY provide a configuration option allowing to truncate properties such as individual message contents, preserving JSON structure. Uploading content to external storage Instrumentations MAY support user-defined in-process hooks to handle content upload. The hook SHOULD operate independently of the opt-in flags that control capturing of gen_ai.system_instructions , gen_ai.input.messages , and gen_ai.output.messages . If such a hook is supported and configured, instrumentations SHOULD invoke it regardless of the span sampling decision with: the instructions, inputs, and outputs object using formats defined in this convention and before they are serialized to JSON string; the span instance The hook implementation SHOULD be able to enrich and modify provided span, instructions, and message objects. If instrumentation is configured to also record gen_ai.system_instructions , gen_ai.input.messages , and gen_ai.output.messages attributes, it SHOULD do it after calling the hook and SHOULD record values that were potentially modified within the hook implementation. The hook API SHOULD be generic. The application or distro is responsible for the hook implementation including the uploading process either in synchronous or asynchronous way, recording references to the uploaded content on the span, handling content in a different way. Application or OpenTelemetry distributions MAY also implement content uploading in the telemetry processing pipeline (in-process or via a collector), based on the gen_ai.system_instructions , gen_ai.input.messages , and gen_ai.output.messages attributes. Given the potential data volume, it is RECOMMENDED to tune batching and export settings accordingly in the OpenTelemetry SDK pipeline. TODO: document a common approach to record references to externally stored content. Check out LLM call examples. Streaming chunks TODO

---

# Semantic conventions for Generative AI events | OpenTelemetry

Source: https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-events/

Semantic conventions for Generative AI events Status: Development Warning Existing GenAI instrumentations that are using v1.36.0 of this document (or prior): SHOULD NOT change the version of the GenAI conventions that they emit by default. Conventions include, but are not limited to, attributes, metric, span and event names, span kind and unit of measure. SHOULD introduce an environment variable OTEL_SEMCONV_STABILITY_OPT_IN as a comma-separated list of category-specific values. The list of values includes: gen_ai_latest_experimental - emit the latest experimental version of GenAI conventions (supported by the instrumentation) and do not emit the old one (v1.36.0 or prior). The default behavior is to continue emitting whatever version of the GenAI conventions the instrumentation was emitting (1.36.0 or prior). This transition plan will be updated to include stable version before the GenAI conventions are marked as stable. GenAI instrumentations MAY capture user inputs sent to the model and responses received from it as events. Note: Events are in-development and not yet available in some languages. Check spec-compliance matrix to see the implementation status in corresponding language. Event: event.gen_ai.client.inference.operation.details Status: Development The event name MUST be gen_ai.client.inference.operation.details . Describes the details of a GenAI completion request including chat history and parameters. This event is opt-in and could be used to store input and output details independently from traces. Attributes: Key Stability Requirement Level Value Type Description Example Values gen_ai.operation.name Development Required string The name of the operation being performed. [1] chat ; generate_content ; text_completion error.type Stable Conditionally Required if the operation ended in an error string Describes a class of error the operation ended with. [2] timeout ; java.net.UnknownHostException ; server_certificate_invalid ; 500 gen_ai.conversation.id Development Conditionally Required when available string The unique identifier for a conversation (session, thread), used to store and correlate messages within this conversation. [3] conv_5j66UpCpwteGg4YSxUnt7lPY gen_ai.output.type Development Conditionally Required [4] string Represents the content type requested by the client. [5] text ; json ; image gen_ai.request.choice.count Development Conditionally Required if available, in the request, and !=1 int The target number of candidate completions to return. 3 gen_ai.request.model Development Conditionally Required If available. string The name of the GenAI model a request is being made to. [6] gpt-4 gen_ai.request.seed Development Conditionally Required if applicable and if the request includes a seed int Requests with same seed value more likely to return same result. 100 server.port Stable Conditionally Required If server.address is set. int GenAI server port. [7] 80 ; 8080 ; 443 gen_ai.request.frequency_penalty Development Recommended double The frequency penalty setting for the GenAI request. 0.1 gen_ai.request.max_tokens Development Recommended int The maximum number of tokens the model generates for a request. 100 gen_ai.request.presence_penalty Development Recommended double The presence penalty setting for the GenAI request. 0.1 gen_ai.request.stop_sequences Development Recommended string[] List of sequences that the model will use to stop generating further tokens. ["forest", "lived"] gen_ai.request.temperature Development Recommended double The temperature setting for the GenAI request. 0.0 gen_ai.request.top_p Development Recommended double The top_p sampling setting for the GenAI request. 1.0 gen_ai.response.finish_reasons Development Recommended string[] Array of reasons the model stopped generating tokens, corresponding to each generation received. ["stop"] ; ["stop", "length"] gen_ai.response.id Development Recommended string The unique identifier for the completion. chatcmpl-123 gen_ai.response.model Development Recommended string The name of the model that generated the response. [8] gpt-4-0613 gen_ai.usage.input_tokens Development Recommended int The number of tokens used in the GenAI input (prompt). 100 gen_ai.usage.output_tokens Development Recommended int The number of tokens used in the GenAI response (completion). 180 server.address Stable Recommended string GenAI server address. [9] example.com ; 10.1.2.80 ; /tmp/my.sock gen_ai.input.messages Development Opt-In any The chat history provided to the model as an input. [10] [ { “role”: “user”, “parts”: [ { “type”: “text”, “content”: “Weather in Paris?" } ] }, { “role”: “assistant”, “parts”: [ { “type”: “tool_call”, “id”: “call_VSPygqKTWdrhaFErNvMV18Yl”, “name”: “get_weather”, “arguments”: { “location”: “Paris” } } ] }, { “role”: “tool”, “parts”: [ { “type”: “tool_call_response”, “id”: " call_VSPygqKTWdrhaFErNvMV18Yl”, “result”: “rainy, 57°F” } ] } ] gen_ai.output.messages Development Opt-In any Messages returned by the model where each message represents a specific model response (choice, candidate). [11] [ { “role”: “assistant”, “parts”: [ { “type”: “text”, “content”: “The weather in Paris is currently rainy with a temperature of 57°F." } ], “finish_reason”: “stop” } ] gen_ai.system_instructions Development Opt-In any The system message or instructions provided to the GenAI model separately from the chat history. [12] [ { “type”: “text”, “content”: “You are an Agent that greet users, always use greetings tool to respond” } ]; [ { “type”: “text”, “content”: “You are a language translator." }, { “type”: “text”, “content”: “Your mission is to translate text in English to French." } ] gen_ai.tool.definitions Development Opt-In any The list of source system tool definitions available to the GenAI agent or model. [13] [ { “type”: “function”, “name”: “get_current_weather”, “description”: “Get the current weather in a given location”, “parameters”: { “type”: “object”, “properties”: { “location”: { “type”: “string”, “description”: “The city and state, e.g. San Francisco, CA” }, “unit”: { “type”: “string”, “enum”: [ “celsius”, “fahrenheit” ] } } }, “required”: [ “location”, “unit” ] } } ] [1] gen_ai.operation.name : If one of the predefined values applies, but specific system uses a different name it's RECOMMENDED to document it in the semantic conventions for specific GenAI system and use system-specific name in the instrumentation. If a different name is not documented, instrumentation libraries SHOULD use applicable predefined value. [2] error.type : The error.type SHOULD match the error code returned by the Generative AI provider or the client library, the canonical name of exception that occurred, or another low-cardinality error identifier. Instrumentations SHOULD document the list of errors they report. [3] gen_ai.conversation.id : Instrumentations SHOULD populate conversation id when they have it readily available for a given operation, for example: when client framework being instrumented manages conversation history (see LlamaIndex chat store) when instrumenting GenAI client libraries that maintain conversation on the backend side (see AWS Bedrock agent sessions, OpenAI Assistant threads) Application developers that manage conversation history MAY add conversation id to GenAI and other spans or logs using custom span or log record processors or hooks provided by instrumentation libraries. [4] gen_ai.output.type : when applicable and if the request includes an output format. [5] gen_ai.output.type : This attribute SHOULD be used when the client requests output of a specific type. The model may return zero or more outputs of this type. This attribute specifies the output modality and not the actual output format. For example, if an image is requested, the actual output could be a URL pointing to an image file. Additional output format details may be recorded in the future in the gen_ai.output.{type}.* attributes. [6] gen_ai.request.model : The name of the GenAI model a request is being made to. If the model is supplied by a vendor, then the value must be the exact name of the model requested. If the model is a fine-tuned custom model, the value should have a more specific name than the base model that's been fine-tuned. [7] server.port : When observed from the client side, and when communicating through an intermediary, server.port SHOULD represent the server port behind any intermediaries, for example proxies, if it's available. [8] gen_ai.response.model : If available. The name of the GenAI model that provided the response. If the model is supplied by a vendor, then the value must be the exact name of the model actually used. If the model is a fine-tuned custom model, the value should have a more specific name than the base model that's been fine-tuned. [9] server.address : When observed from the client side, and when communicating through an intermediary, server.address SHOULD represent the server address behind any intermediaries, for example proxies, if it's available. [10] gen_ai.input.messages : Instrumentations MUST follow Input messages JSON schema. When the attribute is recorded on events, it MUST be recorded in structured form. When recorded on spans, it MAY be recorded as a JSON string if structured format is not supported and SHOULD be recorded in structured form otherwise. Messages MUST be provided in the order they were sent to the model. Instrumentations MAY provide a way for users to filter or truncate input messages. Warning This attribute is likely to contain sensitive information including user/PII data. See Recording content on attributes section for more details. [11] gen_ai.output.messages : Instrumentations MUST follow Output messages JSON schema Each message represents a single output choice/candidate generated by the model. Each message corresponds to exactly one generation (choice/candidate) and vice versa - one choice cannot be split across multiple messages or one message cannot contain parts from multiple choices. When the attribute is recorded on events, it MUST be recorded in structured form. When recorded on spans, it MAY be recorded as a JSON string if structured format is not supported and SHOULD be recorded in structured form otherwise. Instrumentations MAY provide a way for users to filter or truncate output messages. Warning This attribute is likely to contain sensitive information including user/PII data. See Recording content on attributes section for more details. [12] gen_ai.system_instructions : This attribute SHOULD be used when the corresponding provider or API allows to provide system instructions or messages separately from the chat history. Instructions that are part of the chat history SHOULD be recorded in gen_ai.input.messages attribute instead. Instrumentations MUST follow System instructions JSON schema. When recorded on spans, it MAY be recorded as a JSON string if structured format is not supported and SHOULD be recorded in structured form otherwise. Instrumentations MAY provide a way for users to filter or truncate system instructions. Warning This attribute may contain sensitive information. See Recording content on attributes section for more details. [13] gen_ai.tool.definitions : The value of this attribute matches source system tool definition format. It's expected to be an array of objects where each object represents a tool definition. In case a serialized string is available to the instrumentation, the instrumentation SHOULD do the best effort to deserialize it to an array. When recorded on spans, it MAY be recorded as a JSON string if structured format is not supported and SHOULD be recorded in structured form otherwise. Since this attribute could be large, it's NOT RECOMMENDED to populate it by default. Instrumentations MAY provide a way to enable populating this attribute. error.type has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability _OTHER A fallback error value to be used when the instrumentation doesn't define a custom value. Stable gen_ai.operation.name has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability chat Chat completion operation such as OpenAI Chat API Development create_agent Create GenAI agent Development embeddings Embeddings operation such as OpenAI Create embeddings API Development execute_tool Execute a tool Development generate_content Multimodal content generation operation such as Gemini Generate Content Development invoke_agent Invoke GenAI agent Development text_completion Text completions operation such as OpenAI Completions API (Legacy) Development gen_ai.output.type has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability image Image Development json JSON object with known or unknown schema Development speech Speech Development text Plain text Development Event: event.gen_ai.evaluation.result Status: Development The event name MUST be gen_ai.evaluation.result . This event captures the result of evaluating GenAI output for quality, accuracy, or other characteristics. This event SHOULD be parented to GenAI operation span being evaluated when possible or set gen_ai.response.id when span id is not available. Attributes: Key Stability Requirement Level Value Type Description Example Values gen_ai.evaluation.name Development Required string The name of the evaluation metric used for the GenAI response. Relevance ; IntentResolution error.type Stable Conditionally Required if the operation ended in an error string Describes a class of error the operation ended with. [1] timeout ; java.net.UnknownHostException ; server_certificate_invalid ; 500 gen_ai.evaluation.score.label Development Conditionally Required if applicable string Human readable label for evaluation. [2] relevant ; not_relevant ; correct ; incorrect ; pass ; fail gen_ai.evaluation.score.value Development Conditionally Required if applicable double The evaluation score returned by the evaluator. 4.0 gen_ai.evaluation.explanation Development Recommended string A free-form explanation for the assigned score provided by the evaluator. The response is factually accurate but lacks sufficient detail to fully address the question. gen_ai.response.id Development Recommended when available string The unique identifier for the completion. [3] chatcmpl-123 [1] error.type : The error.type SHOULD match the error code returned by the Generative AI Evaluation provider or the client library, the canonical name of exception that occurred, or another low-cardinality error identifier. Instrumentations SHOULD document the list of errors they report. [2] gen_ai.evaluation.score.label : This attribute provides a human-readable interpretation of the evaluation score produced by an evaluator. For example, a score value of 1 could mean “relevant” in one evaluation system and “not relevant” in another, depending on the scoring range and evaluator. The label SHOULD have low cardinality. Possible values depend on the evaluation metric and evaluator used; implementations SHOULD document the possible values. [3] gen_ai.response.id : The unique identifier assigned to the specific completion being evaluated. This attribute helps correlate the evaluation event with the corresponding operation when span id is not available. error.type has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability _OTHER A fallback error value to be used when the instrumentation doesn't define a custom value. Stable

---

# Semantic conventions for generative AI metrics | OpenTelemetry

Source: https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-metrics/

Semantic conventions for generative AI metrics Status: Development Warning Existing GenAI instrumentations that are using v1.36.0 of this document (or prior): SHOULD NOT change the version of the GenAI conventions that they emit by default. Conventions include, but are not limited to, attributes, metric, span and event names, span kind and unit of measure. SHOULD introduce an environment variable OTEL_SEMCONV_STABILITY_OPT_IN as a comma-separated list of category-specific values. The list of values includes: gen_ai_latest_experimental - emit the latest experimental version of GenAI conventions (supported by the instrumentation) and do not emit the old one (v1.36.0 or prior). The default behavior is to continue emitting whatever version of the GenAI conventions the instrumentation was emitting (1.36.0 or prior). This transition plan will be updated to include stable version before the GenAI conventions are marked as stable. Generative AI client metrics The conventions described in this section are specific to Generative AI client applications. Disclaimer: These are initial Generative AI client metric instruments and attributes but more may be added in the future. The following metric instruments describe Generative AI operations. An operation may be a request to an LLM, a function call, or some other distinct action within a larger Generative AI workflow. Individual systems may include additional system-specific attributes. It is recommended to check system-specific documentation, if available. Metric: gen_ai.client.token.usage This metric is recommended when an operation involves the usage of tokens and the count is readily available. For example, if GenAI system returns usage information in the streaming response, it SHOULD be used. Or if GenAI system returns each token independently, instrumentation SHOULD count number of output tokens and record the result. If instrumentation cannot efficiently obtain number of input and/or output tokens, it MAY allow users to enable offline token counting. Otherwise it MUST NOT report usage metric. When systems report both used tokens and billable tokens, instrumentation MUST report billable tokens. This metric SHOULD be specified with ExplicitBucketBoundaries of [1, 4, 16, 64, 256, 1024, 4096, 16384, 65536, 262144, 1048576, 4194304, 16777216, 67108864]. Name Instrument Type Unit (UCUM) Description Stability Entity Associations gen_ai.client.token.usage Histogram {token} Number of input and output tokens used. Development Attributes: Key Stability Requirement Level Value Type Description Example Values gen_ai.operation.name Development Required string The name of the operation being performed. [1] chat ; generate_content ; text_completion gen_ai.provider.name Development Required string The Generative AI provider as identified by the client or server instrumentation. [2] openai ; gcp.gen_ai ; gcp.vertex_ai gen_ai.token.type Development Required string The type of token being counted. input ; output gen_ai.request.model Development Conditionally Required If available. string The name of the GenAI model a request is being made to. gpt-4 server.port Stable Conditionally Required If server.address is set. int GenAI server port. [3] 80 ; 8080 ; 443 gen_ai.response.model Development Recommended string The name of the model that generated the response. gpt-4-0613 server.address Stable Recommended string GenAI server address. [4] example.com ; 10.1.2.80 ; /tmp/my.sock [1] gen_ai.operation.name : If one of the predefined values applies, but specific system uses a different name it's RECOMMENDED to document it in the semantic conventions for specific GenAI system and use system-specific name in the instrumentation. If a different name is not documented, instrumentation libraries SHOULD use applicable predefined value. [2] gen_ai.provider.name : The attribute SHOULD be set based on the instrumentation's best knowledge and may differ from the actual model provider. Multiple providers, including Azure OpenAI, Gemini, and AI hosting platforms are accessible using the OpenAI REST API and corresponding client libraries, but may proxy or host models from different providers. The gen_ai.request.model , gen_ai.response.model , and server.address attributes may help identify the actual system in use. The gen_ai.provider.name attribute acts as a discriminator that identifies the GenAI telemetry format flavor specific to that provider within GenAI semantic conventions. It SHOULD be set consistently with provider-specific attributes and signals. For example, GenAI spans, metrics, and events related to AWS Bedrock should have the gen_ai.provider.name set to aws.bedrock and include applicable aws.bedrock.* attributes and are not expected to include openai.* attributes. [3] server.port : When observed from the client side, and when communicating through an intermediary, server.port SHOULD represent the server port behind any intermediaries, for example proxies, if it's available. [4] server.address : When observed from the client side, and when communicating through an intermediary, server.address SHOULD represent the server address behind any intermediaries, for example proxies, if it's available. gen_ai.operation.name has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability chat Chat completion operation such as OpenAI Chat API Development create_agent Create GenAI agent Development embeddings Embeddings operation such as OpenAI Create embeddings API Development execute_tool Execute a tool Development generate_content Multimodal content generation operation such as Gemini Generate Content Development invoke_agent Invoke GenAI agent Development text_completion Text completions operation such as OpenAI Completions API (Legacy) Development gen_ai.provider.name has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability anthropic Anthropic Development aws.bedrock AWS Bedrock Development azure.ai.inference Azure AI Inference Development azure.ai.openai Azure OpenAI Development cohere Cohere Development deepseek DeepSeek Development gcp.gemini Gemini [5] Development gcp.gen_ai Any Google generative AI endpoint [6] Development gcp.vertex_ai Vertex AI [7] Development groq Groq Development ibm.watsonx.ai IBM Watsonx AI Development mistral_ai Mistral AI Development openai OpenAI Development perplexity Perplexity Development x_ai xAI Development [5]: Used when accessing the 'generativelanguage.googleapis.com' endpoint. Also known as the AI Studio API. [6]: May be used when specific backend is unknown. [7]: Used when accessing the 'aiplatform.googleapis.com' endpoint. gen_ai.token.type has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability input Input tokens (prompt, input, etc.) Development output Output tokens (completion, response, etc.) Development Metric: gen_ai.client.operation.duration This metric is required. This metric SHOULD be specified with ExplicitBucketBoundaries of [0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12, 10.24, 20.48, 40.96, 81.92]. Name Instrument Type Unit (UCUM) Description Stability Entity Associations gen_ai.client.operation.duration Histogram s GenAI operation duration. Development Attributes: Key Stability Requirement Level Value Type Description Example Values gen_ai.operation.name Development Required string The name of the operation being performed. [1] chat ; generate_content ; text_completion gen_ai.provider.name Development Required string The Generative AI provider as identified by the client or server instrumentation. [2] openai ; gcp.gen_ai ; gcp.vertex_ai error.type Stable Conditionally Required if the operation ended in an error string Describes a class of error the operation ended with. [3] timeout ; java.net.UnknownHostException ; server_certificate_invalid ; 500 gen_ai.request.model Development Conditionally Required If available. string The name of the GenAI model a request is being made to. gpt-4 server.port Stable Conditionally Required If server.address is set. int GenAI server port. [4] 80 ; 8080 ; 443 gen_ai.response.model Development Recommended string The name of the model that generated the response. gpt-4-0613 server.address Stable Recommended string GenAI server address. [5] example.com ; 10.1.2.80 ; /tmp/my.sock [1] gen_ai.operation.name : If one of the predefined values applies, but specific system uses a different name it's RECOMMENDED to document it in the semantic conventions for specific GenAI system and use system-specific name in the instrumentation. If a different name is not documented, instrumentation libraries SHOULD use applicable predefined value. [2] gen_ai.provider.name : The attribute SHOULD be set based on the instrumentation's best knowledge and may differ from the actual model provider. Multiple providers, including Azure OpenAI, Gemini, and AI hosting platforms are accessible using the OpenAI REST API and corresponding client libraries, but may proxy or host models from different providers. The gen_ai.request.model , gen_ai.response.model , and server.address attributes may help identify the actual system in use. The gen_ai.provider.name attribute acts as a discriminator that identifies the GenAI telemetry format flavor specific to that provider within GenAI semantic conventions. It SHOULD be set consistently with provider-specific attributes and signals. For example, GenAI spans, metrics, and events related to AWS Bedrock should have the gen_ai.provider.name set to aws.bedrock and include applicable aws.bedrock.* attributes and are not expected to include openai.* attributes. [3] error.type : The error.type SHOULD match the error code returned by the Generative AI provider or the client library, the canonical name of exception that occurred, or another low-cardinality error identifier. Instrumentations SHOULD document the list of errors they report. [4] server.port : When observed from the client side, and when communicating through an intermediary, server.port SHOULD represent the server port behind any intermediaries, for example proxies, if it's available. [5] server.address : When observed from the client side, and when communicating through an intermediary, server.address SHOULD represent the server address behind any intermediaries, for example proxies, if it's available. error.type has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability _OTHER A fallback error value to be used when the instrumentation doesn't define a custom value. Stable gen_ai.operation.name has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability chat Chat completion operation such as OpenAI Chat API Development create_agent Create GenAI agent Development embeddings Embeddings operation such as OpenAI Create embeddings API Development execute_tool Execute a tool Development generate_content Multimodal content generation operation such as Gemini Generate Content Development invoke_agent Invoke GenAI agent Development text_completion Text completions operation such as OpenAI Completions API (Legacy) Development gen_ai.provider.name has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability anthropic Anthropic Development aws.bedrock AWS Bedrock Development azure.ai.inference Azure AI Inference Development azure.ai.openai Azure OpenAI Development cohere Cohere Development deepseek DeepSeek Development gcp.gemini Gemini [6] Development gcp.gen_ai Any Google generative AI endpoint [7] Development gcp.vertex_ai Vertex AI [8] Development groq Groq Development ibm.watsonx.ai IBM Watsonx AI Development mistral_ai Mistral AI Development openai OpenAI Development perplexity Perplexity Development x_ai xAI Development [6]: Used when accessing the 'generativelanguage.googleapis.com' endpoint. Also known as the AI Studio API. [7]: May be used when specific backend is unknown. [8]: Used when accessing the 'aiplatform.googleapis.com' endpoint. Generative AI model server metrics The following metric instruments describe Generative AI model servers' operational metrics. It includes both functional and performance metrics. Metric: gen_ai.server.request.duration This metric is recommended to report the model server latency in terms of time spent per request. This metric SHOULD be specified with ExplicitBucketBoundaries of [0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12, 10.24, 20.48, 40.96, 81.92]. Name Instrument Type Unit (UCUM) Description Stability Entity Associations gen_ai.server.request.duration Histogram s Generative AI server request duration such as time-to-last byte or last output token. Development Attributes: Key Stability Requirement Level Value Type Description Example Values gen_ai.operation.name Development Required string The name of the operation being performed. [1] chat ; generate_content ; text_completion gen_ai.provider.name Development Required string The Generative AI provider as identified by the client or server instrumentation. [2] openai ; gcp.gen_ai ; gcp.vertex_ai error.type Stable Conditionally Required if the operation ended in an error string Describes a class of error the operation ended with. [3] timeout ; java.net.UnknownHostException ; server_certificate_invalid ; 500 gen_ai.request.model Development Conditionally Required If available. string The name of the GenAI model a request is being made to. gpt-4 server.port Stable Conditionally Required If server.address is set. int GenAI server port. [4] 80 ; 8080 ; 443 gen_ai.response.model Development Recommended string The name of the model that generated the response. gpt-4-0613 server.address Stable Recommended string GenAI server address. [5] example.com ; 10.1.2.80 ; /tmp/my.sock [1] gen_ai.operation.name : If one of the predefined values applies, but specific system uses a different name it's RECOMMENDED to document it in the semantic conventions for specific GenAI system and use system-specific name in the instrumentation. If a different name is not documented, instrumentation libraries SHOULD use applicable predefined value. [2] gen_ai.provider.name : The attribute SHOULD be set based on the instrumentation's best knowledge and may differ from the actual model provider. Multiple providers, including Azure OpenAI, Gemini, and AI hosting platforms are accessible using the OpenAI REST API and corresponding client libraries, but may proxy or host models from different providers. The gen_ai.request.model , gen_ai.response.model , and server.address attributes may help identify the actual system in use. The gen_ai.provider.name attribute acts as a discriminator that identifies the GenAI telemetry format flavor specific to that provider within GenAI semantic conventions. It SHOULD be set consistently with provider-specific attributes and signals. For example, GenAI spans, metrics, and events related to AWS Bedrock should have the gen_ai.provider.name set to aws.bedrock and include applicable aws.bedrock.* attributes and are not expected to include openai.* attributes. [3] error.type : The error.type SHOULD match the error code returned by the Generative AI service, the canonical name of exception that occurred, or another low-cardinality error identifier. Instrumentations SHOULD document the list of errors they report. [4] server.port : When observed from the client side, and when communicating through an intermediary, server.port SHOULD represent the server port behind any intermediaries, for example proxies, if it's available. [5] server.address : When observed from the client side, and when communicating through an intermediary, server.address SHOULD represent the server address behind any intermediaries, for example proxies, if it's available. error.type has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability _OTHER A fallback error value to be used when the instrumentation doesn't define a custom value. Stable gen_ai.operation.name has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability chat Chat completion operation such as OpenAI Chat API Development create_agent Create GenAI agent Development embeddings Embeddings operation such as OpenAI Create embeddings API Development execute_tool Execute a tool Development generate_content Multimodal content generation operation such as Gemini Generate Content Development invoke_agent Invoke GenAI agent Development text_completion Text completions operation such as OpenAI Completions API (Legacy) Development gen_ai.provider.name has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability anthropic Anthropic Development aws.bedrock AWS Bedrock Development azure.ai.inference Azure AI Inference Development azure.ai.openai Azure OpenAI Development cohere Cohere Development deepseek DeepSeek Development gcp.gemini Gemini [6] Development gcp.gen_ai Any Google generative AI endpoint [7] Development gcp.vertex_ai Vertex AI [8] Development groq Groq Development ibm.watsonx.ai IBM Watsonx AI Development mistral_ai Mistral AI Development openai OpenAI Development perplexity Perplexity Development x_ai xAI Development [6]: Used when accessing the 'generativelanguage.googleapis.com' endpoint. Also known as the AI Studio API. [7]: May be used when specific backend is unknown. [8]: Used when accessing the 'aiplatform.googleapis.com' endpoint. Metric: gen_ai.server.time_per_output_token This metric is recommended to report the model server latency in terms of time per token generated after the first token for any model servers which support serving LLMs. It is measured by subtracting the time taken to generate the first output token from the request duration and dividing the rest of the duration by the number of output tokens generated after the first token. This is important in measuring the performance of the decode phase of LLM inference. This metric SHOULD be specified with ExplicitBucketBoundaries of [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.75, 1.0, 2.5]. Name Instrument Type Unit (UCUM) Description Stability Entity Associations gen_ai.server.time_per_output_token Histogram s Time per output token generated after the first token for successful responses. Development Attributes: Key Stability Requirement Level Value Type Description Example Values gen_ai.operation.name Development Required string The name of the operation being performed. [1] chat ; generate_content ; text_completion gen_ai.provider.name Development Required string The Generative AI provider as identified by the client or server instrumentation. [2] openai ; gcp.gen_ai ; gcp.vertex_ai gen_ai.request.model Development Conditionally Required If available. string The name of the GenAI model a request is being made to. gpt-4 server.port Stable Conditionally Required If server.address is set. int GenAI server port. [3] 80 ; 8080 ; 443 gen_ai.response.model Development Recommended string The name of the model that generated the response. gpt-4-0613 server.address Stable Recommended string GenAI server address. [4] example.com ; 10.1.2.80 ; /tmp/my.sock [1] gen_ai.operation.name : If one of the predefined values applies, but specific system uses a different name it's RECOMMENDED to document it in the semantic conventions for specific GenAI system and use system-specific name in the instrumentation. If a different name is not documented, instrumentation libraries SHOULD use applicable predefined value. [2] gen_ai.provider.name : The attribute SHOULD be set based on the instrumentation's best knowledge and may differ from the actual model provider. Multiple providers, including Azure OpenAI, Gemini, and AI hosting platforms are accessible using the OpenAI REST API and corresponding client libraries, but may proxy or host models from different providers. The gen_ai.request.model , gen_ai.response.model , and server.address attributes may help identify the actual system in use. The gen_ai.provider.name attribute acts as a discriminator that identifies the GenAI telemetry format flavor specific to that provider within GenAI semantic conventions. It SHOULD be set consistently with provider-specific attributes and signals. For example, GenAI spans, metrics, and events related to AWS Bedrock should have the gen_ai.provider.name set to aws.bedrock and include applicable aws.bedrock.* attributes and are not expected to include openai.* attributes. [3] server.port : When observed from the client side, and when communicating through an intermediary, server.port SHOULD represent the server port behind any intermediaries, for example proxies, if it's available. [4] server.address : When observed from the client side, and when communicating through an intermediary, server.address SHOULD represent the server address behind any intermediaries, for example proxies, if it's available. gen_ai.operation.name has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability chat Chat completion operation such as OpenAI Chat API Development create_agent Create GenAI agent Development embeddings Embeddings operation such as OpenAI Create embeddings API Development execute_tool Execute a tool Development generate_content Multimodal content generation operation such as Gemini Generate Content Development invoke_agent Invoke GenAI agent Development text_completion Text completions operation such as OpenAI Completions API (Legacy) Development gen_ai.provider.name has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability anthropic Anthropic Development aws.bedrock AWS Bedrock Development azure.ai.inference Azure AI Inference Development azure.ai.openai Azure OpenAI Development cohere Cohere Development deepseek DeepSeek Development gcp.gemini Gemini [5] Development gcp.gen_ai Any Google generative AI endpoint [6] Development gcp.vertex_ai Vertex AI [7] Development groq Groq Development ibm.watsonx.ai IBM Watsonx AI Development mistral_ai Mistral AI Development openai OpenAI Development perplexity Perplexity Development x_ai xAI Development [5]: Used when accessing the 'generativelanguage.googleapis.com' endpoint. Also known as the AI Studio API. [6]: May be used when specific backend is unknown. [7]: Used when accessing the 'aiplatform.googleapis.com' endpoint. Metric: gen_ai.server.time_to_first_token This metric is recommended to report the model server latency in terms of time spent to generate the first token of the response for any model servers which support serving LLMs. It helps measure the time spent in the queue and the prefill phase. It is important especially for streaming requests. It is calculated at a request level and is reported as a histogram using the buckets mentioned below. This metric SHOULD be specified with ExplicitBucketBoundaries of [0.001, 0.005, 0.01, 0.02, 0.04, 0.06, 0.08, 0.1, 0.25, 0.5, 0.75, 1.0, 2.5, 5.0, 7.5, 10.0]. Name Instrument Type Unit (UCUM) Description Stability Entity Associations gen_ai.server.time_to_first_token Histogram s Time to generate first token for successful responses. Development Attributes: Key Stability Requirement Level Value Type Description Example Values gen_ai.operation.name Development Required string The name of the operation being performed. [1] chat ; generate_content ; text_completion gen_ai.provider.name Development Required string The Generative AI provider as identified by the client or server instrumentation. [2] openai ; gcp.gen_ai ; gcp.vertex_ai gen_ai.request.model Development Conditionally Required If available. string The name of the GenAI model a request is being made to. gpt-4 server.port Stable Conditionally Required If server.address is set. int GenAI server port. [3] 80 ; 8080 ; 443 gen_ai.response.model Development Recommended string The name of the model that generated the response. gpt-4-0613 server.address Stable Recommended string GenAI server address. [4] example.com ; 10.1.2.80 ; /tmp/my.sock [1] gen_ai.operation.name : If one of the predefined values applies, but specific system uses a different name it's RECOMMENDED to document it in the semantic conventions for specific GenAI system and use system-specific name in the instrumentation. If a different name is not documented, instrumentation libraries SHOULD use applicable predefined value. [2] gen_ai.provider.name : The attribute SHOULD be set based on the instrumentation's best knowledge and may differ from the actual model provider. Multiple providers, including Azure OpenAI, Gemini, and AI hosting platforms are accessible using the OpenAI REST API and corresponding client libraries, but may proxy or host models from different providers. The gen_ai.request.model , gen_ai.response.model , and server.address attributes may help identify the actual system in use. The gen_ai.provider.name attribute acts as a discriminator that identifies the GenAI telemetry format flavor specific to that provider within GenAI semantic conventions. It SHOULD be set consistently with provider-specific attributes and signals. For example, GenAI spans, metrics, and events related to AWS Bedrock should have the gen_ai.provider.name set to aws.bedrock and include applicable aws.bedrock.* attributes and are not expected to include openai.* attributes. [3] server.port : When observed from the client side, and when communicating through an intermediary, server.port SHOULD represent the server port behind any intermediaries, for example proxies, if it's available. [4] server.address : When observed from the client side, and when communicating through an intermediary, server.address SHOULD represent the server address behind any intermediaries, for example proxies, if it's available. gen_ai.operation.name has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability chat Chat completion operation such as OpenAI Chat API Development create_agent Create GenAI agent Development embeddings Embeddings operation such as OpenAI Create embeddings API Development execute_tool Execute a tool Development generate_content Multimodal content generation operation such as Gemini Generate Content Development invoke_agent Invoke GenAI agent Development text_completion Text completions operation such as OpenAI Completions API (Legacy) Development gen_ai.provider.name has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability anthropic Anthropic Development aws.bedrock AWS Bedrock Development azure.ai.inference Azure AI Inference Development azure.ai.openai Azure OpenAI Development cohere Cohere Development deepseek DeepSeek Development gcp.gemini Gemini [5] Development gcp.gen_ai Any Google generative AI endpoint [6] Development gcp.vertex_ai Vertex AI [7] Development groq Groq Development ibm.watsonx.ai IBM Watsonx AI Development mistral_ai Mistral AI Development openai OpenAI Development perplexity Perplexity Development x_ai xAI Development [5]: Used when accessing the 'generativelanguage.googleapis.com' endpoint. Also known as the AI Studio API. [6]: May be used when specific backend is unknown. [7]: Used when accessing the 'aiplatform.googleapis.com' endpoint.

---

# Semantic conventions for generative AI systems | OpenTelemetry

Source: https://opentelemetry.io/docs/specs/semconv/gen-ai/

Semantic conventions for generative AI systems Status: Development Warning Existing GenAI instrumentations that are using v1.36.0 of this document (or prior): SHOULD NOT change the version of the GenAI conventions that they emit by default. Conventions include, but are not limited to, attributes, metric, span and event names, span kind and unit of measure. SHOULD introduce an environment variable OTEL_SEMCONV_STABILITY_OPT_IN as a comma-separated list of category-specific values. The list of values includes: gen_ai_latest_experimental - emit the latest experimental version of GenAI conventions (supported by the instrumentation) and do not emit the old one (v1.36.0 or prior). The default behavior is to continue emitting whatever version of the GenAI conventions the instrumentation was emitting (1.36.0 or prior). This transition plan will be updated to include stable version before the GenAI conventions are marked as stable. Semantic conventions for Generative AI operations are defined for the following signals: Events: Semantic Conventions for Generative AI inputs and outputs - events. Metrics: Semantic Conventions for Generative AI operations - metrics. Model spans: Semantic Conventions for Generative AI model operations - spans. Agent spans: Semantic Conventions for Generative AI agent operations - spans. Technology specific semantic conventions are defined for the following GenAI system: Azure AI Inference: Semantic Conventions for Azure AI Inference. OpenAI: Semantic Conventions for OpenAI. AWS Bedrock: Semantic Conventions for AWS Bedrock. Semantic Conventions for GenAI agent and framework spans Semantic conventions for AWS Bedrock operations Semantic conventions for Azure AI Inference client operations Semantic conventions for Generative AI events LLM call examples Semantic conventions for generative AI metrics Semantic conventions for OpenAI client operations Semantic conventions for generative client AI spans

---

# Semantic conventions for OpenAI client operations | OpenTelemetry

Source: https://opentelemetry.io/docs/specs/semconv/gen-ai/openai/

Semantic conventions for OpenAI client operations Status: Development Warning Existing GenAI instrumentations that are using v1.36.0 of this document (or prior): SHOULD NOT change the version of the GenAI conventions that they emit by default. Conventions include, but are not limited to, attributes, metric, span and event names, span kind and unit of measure. SHOULD introduce an environment variable OTEL_SEMCONV_STABILITY_OPT_IN as a comma-separated list of category-specific values. The list of values includes: gen_ai_latest_experimental - emit the latest experimental version of GenAI conventions (supported by the instrumentation) and do not emit the old one (v1.36.0 or prior). The default behavior is to continue emitting whatever version of the GenAI conventions the instrumentation was emitting (1.36.0 or prior). This transition plan will be updated to include stable version before the GenAI conventions are marked as stable. The Semantic Conventions for OpenAI extend and override the Gen AI Semantic Conventions. Spans gen_ai.provider.name MUST be set to "openai" . Inference Status: Development Semantic Conventions for OpenAI client spans extend and override the semantic conventions for Gen AI Spans. gen_ai.provider.name MUST be set to "openai" and SHOULD be provided at span creation time. Span name SHOULD be {gen_ai.operation.name} {gen_ai.request.model} . Span kind SHOULD be CLIENT . Span status SHOULD follow the Recording Errors document. Attributes: Key Stability Requirement Level Value Type Description Example Values gen_ai.operation.name Development Required string The name of the operation being performed. [1] chat ; generate_content ; text_completion gen_ai.request.model Development Required string The name of the GenAI model a request is being made to. [2] gpt-4 error.type Stable Conditionally Required if the operation ended in an error string Describes a class of error the operation ended with. [3] timeout ; java.net.UnknownHostException ; server_certificate_invalid ; 500 gen_ai.conversation.id Development Conditionally Required when available string The unique identifier for a conversation (session, thread), used to store and correlate messages within this conversation. [4] conv_5j66UpCpwteGg4YSxUnt7lPY gen_ai.output.type Development Conditionally Required [5] string Represents the content type requested by the client. [6] text ; json ; image gen_ai.request.choice.count Development Conditionally Required if available, in the request, and !=1 int The target number of candidate completions to return. 3 gen_ai.request.seed Development Conditionally Required if applicable and if the request includes a seed int Requests with same seed value more likely to return same result. 100 openai.request.service_tier Development Conditionally Required [7] string The service tier requested. May be a specific tier, default, or auto. auto ; default openai.response.service_tier Development Conditionally Required [8] string The service tier used for the response. scale ; default server.port Stable Conditionally Required If server.address is set. int GenAI server port. [9] 80 ; 8080 ; 443 gen_ai.request.frequency_penalty Development Recommended double The frequency penalty setting for the GenAI request. 0.1 gen_ai.request.max_tokens Development Recommended int The maximum number of tokens the model generates for a request. 100 gen_ai.request.presence_penalty Development Recommended double The presence penalty setting for the GenAI request. 0.1 gen_ai.request.stop_sequences Development Recommended string[] List of sequences that the model will use to stop generating further tokens. ["forest", "lived"] gen_ai.request.temperature Development Recommended double The temperature setting for the GenAI request. 0.0 gen_ai.request.top_p Development Recommended double The top_p sampling setting for the GenAI request. 1.0 gen_ai.response.finish_reasons Development Recommended string[] Array of reasons the model stopped generating tokens, corresponding to each generation received. ["stop"] ; ["stop", "length"] gen_ai.response.id Development Recommended string The unique identifier for the completion. chatcmpl-123 gen_ai.response.model Development Recommended string The name of the model that generated the response. [10] gpt-4-0613 gen_ai.usage.input_tokens Development Recommended int The number of tokens used in the GenAI input (prompt). 100 gen_ai.usage.output_tokens Development Recommended int The number of tokens used in the GenAI response (completion). 180 openai.response.system_fingerprint Development Recommended string A fingerprint to track any eventual change in the Generative AI environment. fp_44709d6fcb server.address Stable Recommended string GenAI server address. [11] example.com ; 10.1.2.80 ; /tmp/my.sock gen_ai.input.messages Development Opt-In any The chat history provided to the model as an input. [12] [ { “role”: “user”, “parts”: [ { “type”: “text”, “content”: “Weather in Paris?" } ] }, { “role”: “assistant”, “parts”: [ { “type”: “tool_call”, “id”: “call_VSPygqKTWdrhaFErNvMV18Yl”, “name”: “get_weather”, “arguments”: { “location”: “Paris” } } ] }, { “role”: “tool”, “parts”: [ { “type”: “tool_call_response”, “id”: " call_VSPygqKTWdrhaFErNvMV18Yl”, “result”: “rainy, 57°F” } ] } ] gen_ai.output.messages Development Opt-In any Messages returned by the model where each message represents a specific model response (choice, candidate). [13] [ { “role”: “assistant”, “parts”: [ { “type”: “text”, “content”: “The weather in Paris is currently rainy with a temperature of 57°F." } ], “finish_reason”: “stop” } ] gen_ai.system_instructions Development Opt-In any The system message or instructions provided to the GenAI model separately from the chat history. [14] [ { “type”: “text”, “content”: “You are an Agent that greet users, always use greetings tool to respond” } ]; [ { “type”: “text”, “content”: “You are a language translator." }, { “type”: “text”, “content”: “Your mission is to translate text in English to French." } ] gen_ai.tool.definitions Development Opt-In any The list of source system tool definitions available to the GenAI agent or model. [15] [ { “type”: “function”, “name”: “get_current_weather”, “description”: “Get the current weather in a given location”, “parameters”: { “type”: “object”, “properties”: { “location”: { “type”: “string”, “description”: “The city and state, e.g. San Francisco, CA” }, “unit”: { “type”: “string”, “enum”: [ “celsius”, “fahrenheit” ] } } }, “required”: [ “location”, “unit” ] } } ] [1] gen_ai.operation.name : If one of the predefined values applies, but specific system uses a different name it's RECOMMENDED to document it in the semantic conventions for specific GenAI system and use system-specific name in the instrumentation. If a different name is not documented, instrumentation libraries SHOULD use applicable predefined value. [2] gen_ai.request.model : The name of the GenAI model a request is being made to. If the model is supplied by a vendor, then the value must be the exact name of the model requested. If the model is a fine-tuned custom model, the value should have a more specific name than the base model that's been fine-tuned. [3] error.type : The error.type SHOULD match the error code returned by the Generative AI provider or the client library, the canonical name of exception that occurred, or another low-cardinality error identifier. Instrumentations SHOULD document the list of errors they report. [4] gen_ai.conversation.id : Instrumentations SHOULD populate conversation id when they have it readily available for a given operation, for example: when client framework being instrumented manages conversation history (see LlamaIndex chat store) when instrumenting GenAI client libraries that maintain conversation on the backend side (see AWS Bedrock agent sessions, OpenAI Assistant threads) Application developers that manage conversation history MAY add conversation id to GenAI and other spans or logs using custom span or log record processors or hooks provided by instrumentation libraries. [5] gen_ai.output.type : when applicable and if the request includes an output format. [6] gen_ai.output.type : This attribute SHOULD be set to the output type requested by the client: json for structured outputs with defined or undefined schema image for image output speech for speech output text for plain text output The attribute specifies the output modality and not the actual output format. For example, if an image is requested, the actual output could be a URL pointing to an image file. Additional output format details may be recorded in the future in the gen_ai.output.{type}.* attributes. [7] openai.request.service_tier : if the request includes a service_tier and the value is not 'auto' [8] openai.response.service_tier : if the response was received and includes a service_tier [9] server.port : When observed from the client side, and when communicating through an intermediary, server.port SHOULD represent the server port behind any intermediaries, for example proxies, if it's available. [10] gen_ai.response.model : If available. The name of the GenAI model that provided the response. If the model is supplied by a vendor, then the value must be the exact name of the model actually used. If the model is a fine-tuned custom model, the value should have a more specific name than the base model that's been fine-tuned. [11] server.address : When observed from the client side, and when communicating through an intermediary, server.address SHOULD represent the server address behind any intermediaries, for example proxies, if it's available. [12] gen_ai.input.messages : Instrumentations MUST follow Input messages JSON schema. When the attribute is recorded on events, it MUST be recorded in structured form. When recorded on spans, it MAY be recorded as a JSON string if structured format is not supported and SHOULD be recorded in structured form otherwise. Messages MUST be provided in the order they were sent to the model. Instrumentations MAY provide a way for users to filter or truncate input messages. Warning This attribute is likely to contain sensitive information including user/PII data. See Recording content on attributes section for more details. [13] gen_ai.output.messages : Instrumentations MUST follow Output messages JSON schema Each message represents a single output choice/candidate generated by the model. Each message corresponds to exactly one generation (choice/candidate) and vice versa - one choice cannot be split across multiple messages or one message cannot contain parts from multiple choices. When the attribute is recorded on events, it MUST be recorded in structured form. When recorded on spans, it MAY be recorded as a JSON string if structured format is not supported and SHOULD be recorded in structured form otherwise. Instrumentations MAY provide a way for users to filter or truncate output messages. Warning This attribute is likely to contain sensitive information including user/PII data. See Recording content on attributes section for more details. [14] gen_ai.system_instructions : This attribute SHOULD be used when the corresponding provider or API allows to provide system instructions or messages separately from the chat history. Instructions that are part of the chat history SHOULD be recorded in gen_ai.input.messages attribute instead. Instrumentations MUST follow System instructions JSON schema. When recorded on spans, it MAY be recorded as a JSON string if structured format is not supported and SHOULD be recorded in structured form otherwise. Instrumentations MAY provide a way for users to filter or truncate system instructions. Warning This attribute may contain sensitive information. See Recording content on attributes section for more details. [15] gen_ai.tool.definitions : The value of this attribute matches source system tool definition format. It's expected to be an array of objects where each object represents a tool definition. In case a serialized string is available to the instrumentation, the instrumentation SHOULD do the best effort to deserialize it to an array. When recorded on spans, it MAY be recorded as a JSON string if structured format is not supported and SHOULD be recorded in structured form otherwise. Since this attribute could be large, it's NOT RECOMMENDED to populate it by default. Instrumentations MAY provide a way to enable populating this attribute. error.type has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability _OTHER A fallback error value to be used when the instrumentation doesn't define a custom value. Stable gen_ai.operation.name has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability chat Chat completion operation such as OpenAI Chat API Development create_agent Create GenAI agent Development embeddings Embeddings operation such as OpenAI Create embeddings API Development execute_tool Execute a tool Development generate_content Multimodal content generation operation such as Gemini Generate Content Development invoke_agent Invoke GenAI agent Development text_completion Text completions operation such as OpenAI Completions API (Legacy) Development gen_ai.output.type has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability image Image Development json JSON object with known or unknown schema Development speech Speech Development text Plain text Development openai.request.service_tier has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability auto The system will utilize scale tier credits until they are exhausted. Development default The system will utilize the default scale tier. Development Embeddings See common embeddings span definition. Metrics OpenAI metrics follow Generative AI metrics with the noted additional attributes. Individual systems may include additional system-specific attributes. It is recommended to check system-specific documentation, if available. Metric: gen_ai.client.token.usage Reports the usage of tokens following the common gen_ai.client.token.usage definition. Additional attributes: Attributes: Key Stability Requirement Level Value Type Description Example Values openai.response.service_tier Development Recommended string The service tier used for the response. scale ; default openai.response.system_fingerprint Development Recommended string A fingerprint to track any eventual change in the Generative AI environment. fp_44709d6fcb Metric: gen_ai.client.operation.duration Measures the to complete an operation following the common gen_ai.client.operation.duration definition. Additional attributes: Attributes: Key Stability Requirement Level Value Type Description Example Values openai.response.service_tier Development Recommended string The service tier used for the response. scale ; default openai.response.system_fingerprint Development Recommended string A fingerprint to track any eventual change in the Generative AI environment. fp_44709d6fcb

---

# Semantic conventions for Azure AI Inference client operations | OpenTelemetry

Source: https://opentelemetry.io/docs/specs/semconv/gen-ai/azure-ai-inference/

Semantic conventions for Azure AI Inference client operations Status: Development Warning Existing GenAI instrumentations that are using v1.36.0 of this document (or prior): SHOULD NOT change the version of the GenAI conventions that they emit by default. Conventions include, but are not limited to, attributes, metric, span and event names, span kind and unit of measure. SHOULD introduce an environment variable OTEL_SEMCONV_STABILITY_OPT_IN as a comma-separated list of category-specific values. The list of values includes: gen_ai_latest_experimental - emit the latest experimental version of GenAI conventions (supported by the instrumentation) and do not emit the old one (v1.36.0 or prior). The default behavior is to continue emitting whatever version of the GenAI conventions the instrumentation was emitting (1.36.0 or prior). This transition plan will be updated to include stable version before the GenAI conventions are marked as stable. The Semantic Conventions for Azure AI Inference extend and override the GenAI Semantic Conventions. Spans Inference gen_ai.provider.name MUST be set to "azure.ai.inference" and SHOULD be provided at span creation time. Status: Development Semantic Conventions for Azure AI Inference client spans extend and override the semantic conventions for Gen AI Spans. gen_ai.provider.name MUST be set to "azure.ai.inference" and SHOULD be provided at span creation time. Span name SHOULD be {gen_ai.operation.name} {gen_ai.request.model} when the model name is available and {gen_ai.operation.name} otherwise. Span kind SHOULD be CLIENT . Span status SHOULD follow the Recording Errors document. Attributes: Key Stability Requirement Level Value Type Description Example Values gen_ai.operation.name Development Required string The name of the operation being performed. [1] chat ; generate_content ; text_completion error.type Stable Conditionally Required if the operation ended in an error string Describes a class of error the operation ended with. [2] timeout ; java.net.UnknownHostException ; server_certificate_invalid ; 500 gen_ai.conversation.id Development Conditionally Required when available string The unique identifier for a conversation (session, thread), used to store and correlate messages within this conversation. [3] conv_5j66UpCpwteGg4YSxUnt7lPY gen_ai.output.type Development Conditionally Required [4] string Represents the content type requested by the client. [5] text ; json ; image gen_ai.request.choice.count Development Conditionally Required if available, in the request, and !=1 int The target number of candidate completions to return. 3 gen_ai.request.model Development Conditionally Required If available. string The name of the GenAI model a request is being made to. [6] gpt-4 gen_ai.request.seed Development Conditionally Required if applicable and if the request includes a seed int Requests with same seed value more likely to return same result. 100 server.port Stable Conditionally Required If not default (443). int GenAI server port. [7] 80 ; 8080 ; 443 azure.resource_provider.namespace Development Recommended string Azure Resource Provider Namespace as recognized by the client. [8] Microsoft.CognitiveServices gen_ai.request.frequency_penalty Development Recommended double The frequency penalty setting for the GenAI request. 0.1 gen_ai.request.max_tokens Development Recommended int The maximum number of tokens the model generates for a request. 100 gen_ai.request.presence_penalty Development Recommended double The presence penalty setting for the GenAI request. 0.1 gen_ai.request.stop_sequences Development Recommended string[] List of sequences that the model will use to stop generating further tokens. ["forest", "lived"] gen_ai.request.temperature Development Recommended double The temperature setting for the GenAI request. 0.0 gen_ai.request.top_p Development Recommended double The top_p sampling setting for the GenAI request. 1.0 gen_ai.response.finish_reasons Development Recommended string[] Array of reasons the model stopped generating tokens, corresponding to each generation received. ["stop"] ; ["stop", "length"] gen_ai.response.id Development Recommended string The unique identifier for the completion. chatcmpl-123 gen_ai.response.model Development Recommended string The name of the model that generated the response. [9] gpt-4-0613 gen_ai.usage.input_tokens Development Recommended int The number of prompt tokens as reported in the usage prompt_tokens property of the response. 100 gen_ai.usage.output_tokens Development Recommended int The number of completion tokens as reported in the usage completion_tokens property of the response. 180 server.address Stable Recommended string GenAI server address. [10] example.com ; 10.1.2.80 ; /tmp/my.sock gen_ai.input.messages Development Opt-In any The chat history provided to the model as an input. [11] [ { “role”: “user”, “parts”: [ { “type”: “text”, “content”: “Weather in Paris?" } ] }, { “role”: “assistant”, “parts”: [ { “type”: “tool_call”, “id”: “call_VSPygqKTWdrhaFErNvMV18Yl”, “name”: “get_weather”, “arguments”: { “location”: “Paris” } } ] }, { “role”: “tool”, “parts”: [ { “type”: “tool_call_response”, “id”: " call_VSPygqKTWdrhaFErNvMV18Yl”, “result”: “rainy, 57°F” } ] } ] gen_ai.output.messages Development Opt-In any Messages returned by the model where each message represents a specific model response (choice, candidate). [12] [ { “role”: “assistant”, “parts”: [ { “type”: “text”, “content”: “The weather in Paris is currently rainy with a temperature of 57°F." } ], “finish_reason”: “stop” } ] gen_ai.system_instructions Development Opt-In any The system message or instructions provided to the GenAI model separately from the chat history. [13] [ { “type”: “text”, “content”: “You are an Agent that greet users, always use greetings tool to respond” } ]; [ { “type”: “text”, “content”: “You are a language translator." }, { “type”: “text”, “content”: “Your mission is to translate text in English to French." } ] gen_ai.tool.definitions Development Opt-In any The list of source system tool definitions available to the GenAI agent or model. [14] [ { “type”: “function”, “name”: “get_current_weather”, “description”: “Get the current weather in a given location”, “parameters”: { “type”: “object”, “properties”: { “location”: { “type”: “string”, “description”: “The city and state, e.g. San Francisco, CA” }, “unit”: { “type”: “string”, “enum”: [ “celsius”, “fahrenheit” ] } } }, “required”: [ “location”, “unit” ] } } ] [1] gen_ai.operation.name : If one of the predefined values applies, but specific system uses a different name it's RECOMMENDED to document it in the semantic conventions for specific GenAI system and use system-specific name in the instrumentation. If a different name is not documented, instrumentation libraries SHOULD use applicable predefined value. [2] error.type : The error.type SHOULD match the error code returned by the Generative AI provider or the client library, the canonical name of exception that occurred, or another low-cardinality error identifier. Instrumentations SHOULD document the list of errors they report. [3] gen_ai.conversation.id : Instrumentations SHOULD populate conversation id when they have it readily available for a given operation, for example: when client framework being instrumented manages conversation history (see LlamaIndex chat store) when instrumenting GenAI client libraries that maintain conversation on the backend side (see AWS Bedrock agent sessions, OpenAI Assistant threads) Application developers that manage conversation history MAY add conversation id to GenAI and other spans or logs using custom span or log record processors or hooks provided by instrumentation libraries. [4] gen_ai.output.type : when applicable and if the request includes an output format. [5] gen_ai.output.type : This attribute SHOULD be set to the output type requested by the client: json for structured outputs with defined or undefined schema image for image output speech for speech output text for plain text output The attribute specifies the output modality and not the actual output format. For example, if an image is requested, the actual output could be a URL pointing to an image file. Additional output format details may be recorded in the future in the gen_ai.output.{type}.* attributes. [6] gen_ai.request.model : The name of the GenAI model a request is being made to. If the model is supplied by a vendor, then the value must be the exact name of the model requested. If the model is a fine-tuned custom model, the value should have a more specific name than the base model that's been fine-tuned. [7] server.port : When observed from the client side, and when communicating through an intermediary, server.port SHOULD represent the server port behind any intermediaries, for example proxies, if it's available. [8] azure.resource_provider.namespace : When azure.resource_provider.namespace attribute is populated, it MUST be set to Microsoft.CognitiveServices for all operations performed by Azure AI Inference clients. [9] gen_ai.response.model : If available. The name of the GenAI model that provided the response. If the model is supplied by a vendor, then the value must be the exact name of the model actually used. If the model is a fine-tuned custom model, the value should have a more specific name than the base model that's been fine-tuned. [10] server.address : When observed from the client side, and when communicating through an intermediary, server.address SHOULD represent the server address behind any intermediaries, for example proxies, if it's available. [11] gen_ai.input.messages : Instrumentations MUST follow Input messages JSON schema. When the attribute is recorded on events, it MUST be recorded in structured form. When recorded on spans, it MAY be recorded as a JSON string if structured format is not supported and SHOULD be recorded in structured form otherwise. Messages MUST be provided in the order they were sent to the model. Instrumentations MAY provide a way for users to filter or truncate input messages. Warning This attribute is likely to contain sensitive information including user/PII data. See Recording content on attributes section for more details. [12] gen_ai.output.messages : Instrumentations MUST follow Output messages JSON schema Each message represents a single output choice/candidate generated by the model. Each message corresponds to exactly one generation (choice/candidate) and vice versa - one choice cannot be split across multiple messages or one message cannot contain parts from multiple choices. When the attribute is recorded on events, it MUST be recorded in structured form. When recorded on spans, it MAY be recorded as a JSON string if structured format is not supported and SHOULD be recorded in structured form otherwise. Instrumentations MAY provide a way for users to filter or truncate output messages. Warning This attribute is likely to contain sensitive information including user/PII data. See Recording content on attributes section for more details. [13] gen_ai.system_instructions : This attribute SHOULD be used when the corresponding provider or API allows to provide system instructions or messages separately from the chat history. Instructions that are part of the chat history SHOULD be recorded in gen_ai.input.messages attribute instead. Instrumentations MUST follow System instructions JSON schema. When recorded on spans, it MAY be recorded as a JSON string if structured format is not supported and SHOULD be recorded in structured form otherwise. Instrumentations MAY provide a way for users to filter or truncate system instructions. Warning This attribute may contain sensitive information. See Recording content on attributes section for more details. [14] gen_ai.tool.definitions : The value of this attribute matches source system tool definition format. It's expected to be an array of objects where each object represents a tool definition. In case a serialized string is available to the instrumentation, the instrumentation SHOULD do the best effort to deserialize it to an array. When recorded on spans, it MAY be recorded as a JSON string if structured format is not supported and SHOULD be recorded in structured form otherwise. Since this attribute could be large, it's NOT RECOMMENDED to populate it by default. Instrumentations MAY provide a way to enable populating this attribute. error.type has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability _OTHER A fallback error value to be used when the instrumentation doesn't define a custom value. Stable gen_ai.operation.name has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability chat Chat completion operation such as OpenAI Chat API Development create_agent Create GenAI agent Development embeddings Embeddings operation such as OpenAI Create embeddings API Development execute_tool Execute a tool Development generate_content Multimodal content generation operation such as Gemini Generate Content Development invoke_agent Invoke GenAI agent Development text_completion Text completions operation such as OpenAI Completions API (Legacy) Development gen_ai.output.type has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability image Image Development json JSON object with known or unknown schema Development speech Speech Development text Plain text Development Embeddings See common embedding span definition. Metrics Azure AI Inference metrics follow generic Generative AI metrics.

---

# Semantic conventions for AWS Bedrock operations | OpenTelemetry

Source: https://opentelemetry.io/docs/specs/semconv/gen-ai/aws-bedrock/

Semantic conventions for AWS Bedrock operations Status: Development Warning Existing GenAI instrumentations that are using v1.36.0 of this document (or prior): SHOULD NOT change the version of the GenAI conventions that they emit by default. Conventions include, but are not limited to, attributes, metric, span and event names, span kind and unit of measure. SHOULD introduce an environment variable OTEL_SEMCONV_STABILITY_OPT_IN as a comma-separated list of category-specific values. The list of values includes: gen_ai_latest_experimental - emit the latest experimental version of GenAI conventions (supported by the instrumentation) and do not emit the old one (v1.34.0 or prior). The default behavior is to continue emitting whatever version of the GenAI conventions the instrumentation was emitting (1.34.0 or prior). This transition plan will be updated to include stable version before the GenAI conventions are marked as stable. AWS Bedrock Spans The Semantic Conventions for AWS Bedrock extend and override the semantic conventions for Gen AI Spans. gen_ai.provider.name MUST be set to "aws.bedrock" . These attributes track input data and metadata for a request to an AWS Bedrock model. The attributes include general Generative AI attributes and ones specific the AWS Bedrock. Status: Development Describes an AWS Bedrock operation span. Span kind SHOULD be CLIENT . Span status SHOULD follow the Recording Errors document. Attributes: Key Stability Requirement Level Value Type Description Example Values aws.bedrock.guardrail.id Development Required string The unique identifier of the AWS Bedrock Guardrail. A guardrail helps safeguard and prevent unwanted behavior from model responses or user messages. sgi5gkybzqak gen_ai.operation.name Development Required string The name of the operation being performed. [1] chat ; generate_content ; text_completion gen_ai.provider.name Development Required string The Generative AI provider as identified by the client or server instrumentation. [2] openai ; gcp.gen_ai ; gcp.vertex_ai error.type Stable Conditionally Required if the operation ended in an error string Describes a class of error the operation ended with. [3] timeout ; java.net.UnknownHostException ; server_certificate_invalid ; 500 gen_ai.conversation.id Development Conditionally Required when available string The unique identifier for a conversation (session, thread), used to store and correlate messages within this conversation. [4] conv_5j66UpCpwteGg4YSxUnt7lPY gen_ai.output.type Development Conditionally Required [5] string Represents the content type requested by the client. [6] text ; json ; image gen_ai.request.choice.count Development Conditionally Required if available, in the request, and !=1 int The target number of candidate completions to return. 3 gen_ai.request.model Development Conditionally Required If available. string The name of the GenAI model a request is being made to. [7] gpt-4 gen_ai.request.seed Development Conditionally Required if applicable and if the request includes a seed int Requests with same seed value more likely to return same result. 100 server.port Stable Conditionally Required If server.address is set. int GenAI server port. [8] 80 ; 8080 ; 443 aws.bedrock.knowledge_base.id Development Recommended string The unique identifier of the AWS Bedrock Knowledge base. A knowledge base is a bank of information that can be queried by models to generate more relevant responses and augment prompts. XFWUPB9PAW gen_ai.request.frequency_penalty Development Recommended double The frequency penalty setting for the GenAI request. 0.1 gen_ai.request.max_tokens Development Recommended int The maximum number of tokens the model generates for a request. 100 gen_ai.request.presence_penalty Development Recommended double The presence penalty setting for the GenAI request. 0.1 gen_ai.request.stop_sequences Development Recommended string[] List of sequences that the model will use to stop generating further tokens. ["forest", "lived"] gen_ai.request.temperature Development Recommended double The temperature setting for the GenAI request. 0.0 gen_ai.request.top_k Development Recommended double The top_k sampling setting for the GenAI request. 1.0 gen_ai.request.top_p Development Recommended double The top_p sampling setting for the GenAI request. 1.0 gen_ai.response.finish_reasons Development Recommended string[] Array of reasons the model stopped generating tokens, corresponding to each generation received. ["stop"] ; ["stop", "length"] gen_ai.response.id Development Recommended string The unique identifier for the completion. chatcmpl-123 gen_ai.response.model Development Recommended string The name of the model that generated the response. [9] gpt-4-0613 gen_ai.usage.input_tokens Development Recommended int The number of tokens used in the GenAI input (prompt). 100 gen_ai.usage.output_tokens Development Recommended int The number of tokens used in the GenAI response (completion). 180 server.address Stable Recommended string GenAI server address. [10] example.com ; 10.1.2.80 ; /tmp/my.sock gen_ai.input.messages Development Opt-In any The chat history provided to the model as an input. [11] [ { “role”: “user”, “parts”: [ { “type”: “text”, “content”: “Weather in Paris?" } ] }, { “role”: “assistant”, “parts”: [ { “type”: “tool_call”, “id”: “call_VSPygqKTWdrhaFErNvMV18Yl”, “name”: “get_weather”, “arguments”: { “location”: “Paris” } } ] }, { “role”: “tool”, “parts”: [ { “type”: “tool_call_response”, “id”: " call_VSPygqKTWdrhaFErNvMV18Yl”, “result”: “rainy, 57°F” } ] } ] gen_ai.output.messages Development Opt-In any Messages returned by the model where each message represents a specific model response (choice, candidate). [12] [ { “role”: “assistant”, “parts”: [ { “type”: “text”, “content”: “The weather in Paris is currently rainy with a temperature of 57°F." } ], “finish_reason”: “stop” } ] gen_ai.system_instructions Development Opt-In any The system message or instructions provided to the GenAI model separately from the chat history. [13] [ { “type”: “text”, “content”: “You are an Agent that greet users, always use greetings tool to respond” } ]; [ { “type”: “text”, “content”: “You are a language translator." }, { “type”: “text”, “content”: “Your mission is to translate text in English to French." } ] gen_ai.tool.definitions Development Opt-In any The list of source system tool definitions available to the GenAI agent or model. [14] [ { “type”: “function”, “name”: “get_current_weather”, “description”: “Get the current weather in a given location”, “parameters”: { “type”: “object”, “properties”: { “location”: { “type”: “string”, “description”: “The city and state, e.g. San Francisco, CA” }, “unit”: { “type”: “string”, “enum”: [ “celsius”, “fahrenheit” ] } } }, “required”: [ “location”, “unit” ] } } ] [1] gen_ai.operation.name : If one of the predefined values applies, but specific system uses a different name it's RECOMMENDED to document it in the semantic conventions for specific GenAI system and use system-specific name in the instrumentation. If a different name is not documented, instrumentation libraries SHOULD use applicable predefined value. [2] gen_ai.provider.name : The attribute SHOULD be set based on the instrumentation's best knowledge and may differ from the actual model provider. Multiple providers, including Azure OpenAI, Gemini, and AI hosting platforms are accessible using the OpenAI REST API and corresponding client libraries, but may proxy or host models from different providers. The gen_ai.request.model , gen_ai.response.model , and server.address attributes may help identify the actual system in use. The gen_ai.provider.name attribute acts as a discriminator that identifies the GenAI telemetry format flavor specific to that provider within GenAI semantic conventions. It SHOULD be set consistently with provider-specific attributes and signals. For example, GenAI spans, metrics, and events related to AWS Bedrock should have the gen_ai.provider.name set to aws.bedrock and include applicable aws.bedrock.* attributes and are not expected to include openai.* attributes. [3] error.type : The error.type SHOULD match the error code returned by the Generative AI provider or the client library, the canonical name of exception that occurred, or another low-cardinality error identifier. Instrumentations SHOULD document the list of errors they report. [4] gen_ai.conversation.id : Instrumentations SHOULD populate conversation id when they have it readily available for a given operation, for example: when client framework being instrumented manages conversation history (see LlamaIndex chat store) when instrumenting GenAI client libraries that maintain conversation on the backend side (see AWS Bedrock agent sessions, OpenAI Assistant threads) Application developers that manage conversation history MAY add conversation id to GenAI and other spans or logs using custom span or log record processors or hooks provided by instrumentation libraries. [5] gen_ai.output.type : when applicable and if the request includes an output format. [6] gen_ai.output.type : This attribute SHOULD be used when the client requests output of a specific type. The model may return zero or more outputs of this type. This attribute specifies the output modality and not the actual output format. For example, if an image is requested, the actual output could be a URL pointing to an image file. Additional output format details may be recorded in the future in the gen_ai.output.{type}.* attributes. [7] gen_ai.request.model : The name of the GenAI model a request is being made to. If the model is supplied by a vendor, then the value must be the exact name of the model requested. If the model is a fine-tuned custom model, the value should have a more specific name than the base model that's been fine-tuned. [8] server.port : When observed from the client side, and when communicating through an intermediary, server.port SHOULD represent the server port behind any intermediaries, for example proxies, if it's available. [9] gen_ai.response.model : If available. The name of the GenAI model that provided the response. If the model is supplied by a vendor, then the value must be the exact name of the model actually used. If the model is a fine-tuned custom model, the value should have a more specific name than the base model that's been fine-tuned. [10] server.address : When observed from the client side, and when communicating through an intermediary, server.address SHOULD represent the server address behind any intermediaries, for example proxies, if it's available. [11] gen_ai.input.messages : Instrumentations MUST follow Input messages JSON schema. When the attribute is recorded on events, it MUST be recorded in structured form. When recorded on spans, it MAY be recorded as a JSON string if structured format is not supported and SHOULD be recorded in structured form otherwise. Messages MUST be provided in the order they were sent to the model. Instrumentations MAY provide a way for users to filter or truncate input messages. Warning This attribute is likely to contain sensitive information including user/PII data. See Recording content on attributes section for more details. [12] gen_ai.output.messages : Instrumentations MUST follow Output messages JSON schema Each message represents a single output choice/candidate generated by the model. Each message corresponds to exactly one generation (choice/candidate) and vice versa - one choice cannot be split across multiple messages or one message cannot contain parts from multiple choices. When the attribute is recorded on events, it MUST be recorded in structured form. When recorded on spans, it MAY be recorded as a JSON string if structured format is not supported and SHOULD be recorded in structured form otherwise. Instrumentations MAY provide a way for users to filter or truncate output messages. Warning This attribute is likely to contain sensitive information including user/PII data. See Recording content on attributes section for more details. [13] gen_ai.system_instructions : This attribute SHOULD be used when the corresponding provider or API allows to provide system instructions or messages separately from the chat history. Instructions that are part of the chat history SHOULD be recorded in gen_ai.input.messages attribute instead. Instrumentations MUST follow System instructions JSON schema. When recorded on spans, it MAY be recorded as a JSON string if structured format is not supported and SHOULD be recorded in structured form otherwise. Instrumentations MAY provide a way for users to filter or truncate system instructions. Warning This attribute may contain sensitive information. See Recording content on attributes section for more details. [14] gen_ai.tool.definitions : The value of this attribute matches source system tool definition format. It's expected to be an array of objects where each object represents a tool definition. In case a serialized string is available to the instrumentation, the instrumentation SHOULD do the best effort to deserialize it to an array. When recorded on spans, it MAY be recorded as a JSON string if structured format is not supported and SHOULD be recorded in structured form otherwise. Since this attribute could be large, it's NOT RECOMMENDED to populate it by default. Instrumentations MAY provide a way to enable populating this attribute. error.type has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability _OTHER A fallback error value to be used when the instrumentation doesn't define a custom value. Stable gen_ai.operation.name has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability chat Chat completion operation such as OpenAI Chat API Development create_agent Create GenAI agent Development embeddings Embeddings operation such as OpenAI Create embeddings API Development execute_tool Execute a tool Development generate_content Multimodal content generation operation such as Gemini Generate Content Development invoke_agent Invoke GenAI agent Development text_completion Text completions operation such as OpenAI Completions API (Legacy) Development gen_ai.output.type has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability image Image Development json JSON object with known or unknown schema Development speech Speech Development text Plain text Development gen_ai.provider.name has the following list of well-known values. If one of them applies, then the respective value MUST be used; otherwise, a custom value MAY be used. Value Description Stability anthropic Anthropic Development aws.bedrock AWS Bedrock Development azure.ai.inference Azure AI Inference Development azure.ai.openai Azure OpenAI Development cohere Cohere Development deepseek DeepSeek Development gcp.gemini Gemini [15] Development gcp.gen_ai Any Google generative AI endpoint [16] Development gcp.vertex_ai Vertex AI [17] Development groq Groq Development ibm.watsonx.ai IBM Watsonx AI Development mistral_ai Mistral AI Development openai OpenAI Development perplexity Perplexity Development x_ai xAI Development [15]: Used when accessing the 'generativelanguage.googleapis.com' endpoint. Also known as the AI Studio API. [16]: May be used when specific backend is unknown. [17]: Used when accessing the 'aiplatform.googleapis.com' endpoint.
